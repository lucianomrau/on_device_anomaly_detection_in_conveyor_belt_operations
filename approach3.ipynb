{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook corresponding to the \"Approach-3\" presented in the paper.\n",
    "\n",
    "This is the same approach used in the [\"Tinyml anomaly detection for industrial machines with periodic duty cycles\" (Sensor Application Symposium 2024)](https://ieeexplore.ieee.org/abstract/document/10636584/), and serves as the baseline experiment.\n",
    "\n",
    "Two experiments are carried on:\n",
    "1) As in the SAS2024, the performance is evaluated in leave-one-month-out CV in the original 4 months (called DS1).\n",
    "2) The generalization is evaluated using the whole DS1 for training and the whole DS2 for testing.\n",
    "\n",
    "This approach uses two classifiers, one for the internal-states and another one for the duty-cycles.\n",
    "Therefore, each experiment is divided in four part because some classifier have been trained with different seed initializers. The four combinations are:\n",
    "\n",
    " (1) Internal-state classifier with seed -> Duty-cycle classifier with seed.\n",
    "\n",
    " (2) Internal-state classifier without seed -> Duty-cycle classifier with seed.\n",
    "\n",
    " (3) Internal-state classifier with seed -> Duty-cycle classifier without seed.\n",
    "\n",
    " (4) Internal-state classifier without seed -> Duty-cycle classifier without seed.\n",
    "\n",
    "\n",
    " The predicted state labels are loaded from the files generated in the approach-2. This avoid train again the same classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_functions import *\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data, extract feature and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"../../../data/\"\n",
    "#first 4 months of data (DS1)\n",
    "data_csv_jun21 = read_month_data(directory+'Confidential_Drive_data_Jun2021.csv',1)\n",
    "data_csv_okt21 = read_month_data(directory+'Confidential_Drive_data_Okt2021.csv',1)\n",
    "data_csv_jan22 = read_month_data(directory+'Confidential_Drive_data_Jan2022.csv',1)\n",
    "data_csv_april22 = read_month_data(directory+'Confidential_Drive_data_April2022.csv',1)\n",
    "#new 4 months (DS2)\n",
    "data_csv_jun23 = read_month_data(directory+'Confidential_Drive_data_June2023_Drift20.csv')\n",
    "data_csv_aug23 = read_month_data(directory+'Confidential_Drive_data_Aug2023_Drift20.csv')\n",
    "data_csv_okt23 = read_month_data(directory+'Confidential_Drive_data_Oct2023_Drift20.csv')\n",
    "data_csv_dec23 = read_month_data(directory+'Confidential_Drive_data_Dec2023_Drift20.csv')\n",
    "\n",
    "#re-order the column name to be consistent with the previous csv files\n",
    "desired_order=[\"High-pressure\",\"Low-pressure\",\"Speed\"]\n",
    "data_csv_jun23=data_csv_jun23[desired_order]\n",
    "data_csv_aug23=data_csv_aug23[desired_order]\n",
    "data_csv_okt23=data_csv_okt23[desired_order]\n",
    "data_csv_dec23=data_csv_dec23[desired_order]\n",
    "\n",
    "#These data has duplicated entries\n",
    "data_csv_okt23 = data_csv_okt23[~data_csv_okt23.index.duplicated(keep='first')]\n",
    "\n",
    "# round to zero speed less than zero\n",
    "data_csv_jun21.loc[data_csv_jun21['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_okt21.loc[data_csv_okt21['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_jan22.loc[data_csv_jan22['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_april22.loc[data_csv_april22['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_jun23.loc[data_csv_jun23['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_aug23.loc[data_csv_aug23['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_okt23.loc[data_csv_okt23['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_dec23.loc[data_csv_dec23['Speed'] < 0 , 'Speed'] = 0\n",
    "\n",
    "\n",
    "# complete the dataset with missing values\n",
    "full_timestamp = pd.date_range(start = data_csv_jun21.index[0], end = data_csv_jun21.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_jun21 = data_csv_jun21.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_okt21.index[0], end = data_csv_okt21.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_okt21 = data_csv_okt21.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_jan22.index[0], end = data_csv_jan22.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_jan22 = data_csv_jan22.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_april22.index[0], end = data_csv_april22.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_april22 = data_csv_april22.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_jun23.index[0], end = data_csv_jun23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_jun23 = data_csv_jun23.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_aug23.index[0], end = data_csv_aug23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_aug23 = data_csv_aug23.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_okt23.index[0], end = data_csv_okt23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_okt23 = data_csv_okt23.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_dec23.index[0], end = data_csv_dec23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_dec23 = data_csv_dec23.reindex(full_timestamp)\n",
    "\n",
    "\n",
    "\n",
    "#use linear interpolation for the NaN missing values\n",
    "interpolate_values(data_csv_jun21)\n",
    "interpolate_values(data_csv_okt21)\n",
    "interpolate_values(data_csv_jan22)\n",
    "interpolate_values(data_csv_april22)\n",
    "interpolate_values(data_csv_jun23)\n",
    "interpolate_values(data_csv_aug23)\n",
    "interpolate_values(data_csv_okt23)\n",
    "interpolate_values(data_csv_dec23)\n",
    "\n",
    "del desired_order, directory, full_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_csv = [data_csv_jun21,data_csv_okt21,data_csv_jan22,data_csv_april22,data_csv_jun23,data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "for data in list_data_csv:\n",
    "    extract_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ground truth reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files from imagimob\n",
    "directory=\"../../data/\"\n",
    "column_interest=['Time(Seconds)' , 'Length(Seconds)',\"Label(string)\"]\n",
    "\n",
    "#read labels of states\n",
    "file_imagimob_1 = pd.read_csv(directory+\"April_2022/Label.label\",usecols=column_interest)\n",
    "file_imagimob_2 = pd.read_csv(directory+\"Jan_2022/Label.label\",usecols=column_interest)\n",
    "file_imagimob_3 = pd.read_csv(directory+\"Jun_2021/Label.label\",usecols=column_interest)\n",
    "file_imagimob_4 = pd.read_csv(directory+\"Okt_2021/Label.label\",usecols=column_interest)\n",
    "\n",
    "timestamps_april2022 = df_timestamps(file_imagimob_1)\n",
    "timestamps_jan2022 = df_timestamps(file_imagimob_2)\n",
    "timestamps_jun2021 = df_timestamps(file_imagimob_3)\n",
    "timestamps_okt2021 = df_timestamps(file_imagimob_4)\n",
    "\n",
    "#read labels of duty-cycle\n",
    "file_imagimob_1 = pd.read_csv(directory+\"April_2022/Label_cycle.label\",usecols=column_interest)\n",
    "file_imagimob_2 = pd.read_csv(directory+\"Jan_2022/Label_cycle.label\",usecols=column_interest)\n",
    "file_imagimob_3 = pd.read_csv(directory+\"Jun_2021/Label_cycle.label\",usecols=column_interest)\n",
    "file_imagimob_4 = pd.read_csv(directory+\"Okt_2021/Label_cycle.label\",usecols=column_interest)\n",
    "\n",
    "timestamps_cycle_april2022 = df_timestamps(file_imagimob_1)\n",
    "timestamps_cycle_jan2022 = df_timestamps(file_imagimob_2)\n",
    "timestamps_cycle_jun2021 = df_timestamps(file_imagimob_3)\n",
    "timestamps_cycle_okt2021 = df_timestamps(file_imagimob_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate vector with the labels of reference (states)\n",
    "downsampled_freq='1T'\n",
    "true_label_april22 = ndarray_labels(datetime.datetime(2022, 4, 1),datetime.datetime(2022, 5, 1),timestamps_april2022,downsampled_freq)\n",
    "true_label_jan22 = ndarray_labels(datetime.datetime(2021, 12, 21),datetime.datetime(2022, 1, 21),timestamps_jan2022,downsampled_freq)\n",
    "true_label_jun21 = ndarray_labels(datetime.datetime(2021, 6, 1),datetime.datetime(2021, 7, 1),timestamps_jun2021,downsampled_freq)\n",
    "true_label_okt21 = ndarray_labels(datetime.datetime(2021, 10, 1),datetime.datetime(2021, 11, 1),timestamps_okt2021,downsampled_freq)\n",
    "\n",
    "#generate vector with the labels of reference (duty-cycle)\n",
    "true_label_cycle_april22 = ndarray_labels(datetime.datetime(2022, 4, 1),datetime.datetime(2022, 5, 1),timestamps_cycle_april2022,downsampled_freq)\n",
    "true_label_cycle_jan22 = ndarray_labels(datetime.datetime(2021, 12, 21),datetime.datetime(2022, 1, 21),timestamps_cycle_jan2022,downsampled_freq)\n",
    "true_label_cycle_jun21 = ndarray_labels(datetime.datetime(2021, 6, 1),datetime.datetime(2021, 7, 1),timestamps_cycle_jun2021,downsampled_freq)\n",
    "true_label_cycle_okt21 = ndarray_labels(datetime.datetime(2021, 10, 1),datetime.datetime(2021, 11, 1),timestamps_cycle_okt2021,downsampled_freq)\n",
    "\n",
    "true_label_cycle_april22 = np.where(true_label_cycle_april22 == None, 'No_cycle', true_label_cycle_april22)\n",
    "true_label_cycle_jan22 = np.where(true_label_cycle_jan22 == None, 'No_cycle', true_label_cycle_jan22)\n",
    "true_label_cycle_jun21 = np.where(true_label_cycle_jun21 == None, 'No_cycle', true_label_cycle_jun21)\n",
    "true_label_cycle_okt21 = np.where(true_label_cycle_okt21 == None, 'No_cycle', true_label_cycle_okt21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imput ground-truth duty-cycle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files from imagimob\n",
    "directory=\"../../data/\"\n",
    "#read labels of duty-cycle\n",
    "labels_jun21 = import_cycle_labels(directory+\"Jun_2021/Label_cycle.label\")\n",
    "labels_okt21 = import_cycle_labels(directory+\"Okt_2021/Label_cycle.label\")\n",
    "labels_jan22 = import_cycle_labels(directory+\"Jan_2022/Label_cycle.label\")\n",
    "labels_april22 = import_cycle_labels(directory+\"April_2022/Label_cycle.label\")\n",
    "labels_jun23 = import_cycle_labels(directory+\"June_23/Label_cycle.label\")\n",
    "labels_aug23 = import_cycle_labels(directory+\"Aug_23/Label_cycle.label\")\n",
    "labels_okt23 = import_cycle_labels(directory+\"Okt_23/Label_cycle.label\")\n",
    "labels_dec23 = import_cycle_labels(directory+\"Dec_23/Label_cycle.label\")\n",
    "\n",
    "for data in [labels_jun23,labels_aug23,labels_okt23,labels_dec23]:\n",
    "    replace_labels_cycles(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_jun21 [true_label_jun21=='E']='B'\n",
    "true_label_okt21 [true_label_okt21=='E']='B'\n",
    "true_label_jan22 [true_label_jan22=='E']='B'\n",
    "true_label_april22 [true_label_april22=='E']='B'\n",
    "\n",
    "data_DS1=[data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22]\n",
    "data_DS2=[data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "\n",
    "true_state_labels_DS1=[true_label_jun21, true_label_okt21,true_label_jan22,true_label_april22]\n",
    "timestamps_cycle_DS1 = [timestamps_cycle_jun2021,timestamps_cycle_okt2021,timestamps_cycle_jan2022,timestamps_cycle_april2022]\n",
    "\n",
    "df_testset_DS2= pd.concat(data_DS2)\n",
    "\n",
    "file_name_states_DS1= [\"jun2021_state.txt\" ,\"okt2021_state.txt\",\"jan2022_state.txt\",\"april2022_state.txt\"]\n",
    "file_name_cycles_DS1= [\"jun2021_cycle.txt\" ,\"okt2021_cycle.txt\",\"jan2022_cycle.txt\",\"april2022_cycle.txt\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "dir_exp1 = \"./results/approach3/DS1/\"\n",
    "dir_exp2 = \"./results/approach3/DS2/\"\n",
    "\n",
    "flag_save_results=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete not requires variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del timestamps_april2022, timestamps_jan2022, timestamps_jun2021, timestamps_okt2021\n",
    "del file_imagimob_1,file_imagimob_2,file_imagimob_3,file_imagimob_4, column_interest, directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define multiples variables\n",
    "delta = pd.Timedelta(minutes=3)\n",
    "\n",
    "# Encode the non-consecutive labels and the labels for the classifier\n",
    "le_cycle = LabelEncoder()\n",
    "le_cycle.fit(['Normal','Abnormal'])\n",
    "le_state = LabelEncoder()\n",
    "le_state.fit(['Z','A','B','C','D','None'])\n",
    "\n",
    "scaler_cycle = MinMaxScaler()\n",
    "\n",
    "dir_exp1_approach2 = \"./results/approach2/DS1/\"\n",
    "dir_exp2_approach2 = \"./results/approach2/DS2/\"\n",
    "dir_exp1_approach3 = \"./results/approach3/DS1/\"\n",
    "dir_exp2_approach3 = \"./results/approach3/DS2/\"\n",
    "\n",
    "flag_save_results=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Train/test on DS1 using leave-one-month CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_month_out_data_split(i,data_DS1,true_state_labels_DS1,delta):\n",
    "    dfs_ref_state= pd.concat([data for j, data in enumerate(data_DS1) if j != i])\n",
    "    dfs_ref_state[\"ref_label\"]= np.concatenate([data for j, data in enumerate(true_state_labels_DS1) if j != i])\n",
    "\n",
    "    dfs_ref_cycle = pd.concat([timestamps for j, timestamps in enumerate(timestamps_cycle_DS1) if j != i])\n",
    "\n",
    "    train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"ref_label\")\n",
    "\n",
    "    # # Determine the maximum sequence length for padding and add some extra value\n",
    "    max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "    # # Encode the sequences for training\n",
    "    train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "    X_res = pad_sequences(train_features, max_sequence_length)\n",
    "    y_res = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "\n",
    "    unique_values, counts = np.unique(y_res, return_counts=True)\n",
    "    value_counts = dict(zip(unique_values, counts))\n",
    "    value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "    print(\"Value class-counts in Balanced dataset:\",value_counts)\n",
    "    print(\"Value class-porcentage in Balanced dataset:\",value_porcentages)\n",
    "\n",
    "    x_test=data_DS1[i].copy()\n",
    "    x_test['detected_cycles']=x_test[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    x_test[\"ref_state_label\"]=true_state_labels_DS1[i]\n",
    "    \n",
    "    return X_res, y_res, x_test ,max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results path depend on if the classifier use or not the seed parameters, there are 4 combination of classifiers: \"classifiers_state\" w/ and w/o seed and \"classifier_cycle\" w/ and w/o seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) classifier_cycle with seed - classifier_state with seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "for seed_state in seeds:\n",
    "    for classifier_state in classifiers:\n",
    "        folder_path = dir_exp1_approach2+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "        aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "        recog_state_labels_DS1=[recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22]                        \n",
    "\n",
    "        for i in range(len(data_DS1)):\n",
    "            x_res, y_res,x_test,max_sequence_length = leave_one_month_out_data_split(i,data_DS1,recog_state_labels_DS1,delta)\n",
    "\n",
    "            for classifier_cycle in classifiers:\n",
    "                for seed_cycle in seeds:\n",
    "                    clf_duty = train_cycle_classifier(classifier_cycle,x_res, y_res,seed_cycle)\n",
    "                    x_test[\"recognized_label\"] = recog_state_labels_DS1[i]\n",
    "\n",
    "                    detection_x_test = boundaries_cycles(x_test)\n",
    "\n",
    "                    data_sequences = find_non_consecutive_sequences(x_test, detection_x_test,delta,column_name=\"recognized_label\")\n",
    "                    test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in data_sequences]\n",
    "                    test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "                    #test\n",
    "                    y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "                    # save the results in a file\n",
    "                    df_temp= pd.DataFrame()\n",
    "                    df_temp[\"start\"]=detection_x_test[\"start\"]\n",
    "                    df_temp[\"end\"]=detection_x_test[\"end\"]\n",
    "                    df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "                    if flag_save_results:\n",
    "                        folder_path = dir_exp1_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "                        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                        create_reference_label_file(folder_path+file_name_cycles_DS1[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) classifier_cycle with seed - classifier_state without seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "classifiers_cycle=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "classifiers_state=[\"xgboost\",\"nb\"]\n",
    "\n",
    "\n",
    "for classifier_state in classifiers_state:\n",
    "    folder_path = dir_exp1_approach2+classifier_state+\"/\"\n",
    "    aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "    recog_state_labels_DS1=[recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22]                        \n",
    "\n",
    "    for i in range(len(data_DS1)):\n",
    "        x_res, y_res,x_test,max_sequence_length = leave_one_month_out_data_split(i,data_DS1,recog_state_labels_DS1,delta)\n",
    "        \n",
    "        for seed_cycle in seeds:\n",
    "            for classifier_cycle in classifiers_cycle:\n",
    "                clf_duty = train_cycle_classifier(classifier_cycle,x_res, y_res,seed_cycle)\n",
    "                x_test[\"recognized_label\"] = recog_state_labels_DS1[i]\n",
    "\n",
    "                detection_x_test = boundaries_cycles(x_test)\n",
    "\n",
    "                data_sequences = find_non_consecutive_sequences(x_test, detection_x_test,delta,column_name=\"recognized_label\")\n",
    "                test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in data_sequences]\n",
    "                test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "                #test\n",
    "                y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "                # save the results in a file\n",
    "                df_temp= pd.DataFrame()\n",
    "                df_temp[\"start\"]=detection_x_test[\"start\"]\n",
    "                df_temp[\"end\"]=detection_x_test[\"end\"]\n",
    "                df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "                if flag_save_results:\n",
    "                    folder_path = dir_exp1_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                    create_reference_label_file(folder_path+file_name_cycles_DS1[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) classifier_cycle without seed - classifier_state with seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "classifiers_cycle=[\"xgboost\",\"nb\"]\n",
    "classifiers_state=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "for seed in seeds:\n",
    "    for classifier_state in classifiers_state:\n",
    "        folder_path = dir_exp1_approach2+classifier_state+\"/\"+str(seed)+\"/\"\n",
    "        aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "        recog_state_labels_DS1=[recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22]                        \n",
    "\n",
    "        for i in range(len(data_DS1)):\n",
    "            x_res, y_res,x_test,max_sequence_length = leave_one_month_out_data_split(i,data_DS1,recog_state_labels_DS1,delta)\n",
    "\n",
    "            for classifier_cycle in classifiers_cycle:\n",
    "                clf_duty = train_cycle_classifier(classifier_cycle,x_res, y_res)\n",
    "                x_test[\"recognized_label\"] = recog_state_labels_DS1[i]\n",
    "\n",
    "                detection_x_test = boundaries_cycles(x_test)\n",
    "\n",
    "                data_sequences = find_non_consecutive_sequences(x_test, detection_x_test,delta,column_name=\"recognized_label\")\n",
    "                test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in data_sequences]\n",
    "                test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "                #test\n",
    "                y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "                # save the results in a file\n",
    "                df_temp= pd.DataFrame()\n",
    "                df_temp[\"start\"]=detection_x_test[\"start\"]\n",
    "                df_temp[\"end\"]=detection_x_test[\"end\"]\n",
    "                df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "                if flag_save_results:\n",
    "                    folder_path = dir_exp1_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"+str(seed)+\"/\"\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                    create_reference_label_file(folder_path+file_name_cycles_DS1[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) classifier_cycle without seed - classifier_state without seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "\n",
    "for classifier_state in classifiers:\n",
    "    folder_path = dir_exp1_approach2+classifier_state+\"/\"\n",
    "    aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "    recog_state_labels_DS1=[recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22]                        \n",
    "\n",
    "    for i in range(len(data_DS1)):\n",
    "        x_res, y_res,x_test,max_sequence_length = leave_one_month_out_data_split(i,data_DS1,recog_state_labels_DS1,delta)\n",
    "\n",
    "        for classifier_cycle in classifiers:\n",
    "            clf_duty = train_cycle_classifier(classifier_cycle,x_res, y_res)\n",
    "            x_test[\"recognized_label\"] = recog_state_labels_DS1[i]\n",
    "\n",
    "            detection_x_test = boundaries_cycles(x_test)\n",
    "\n",
    "            data_sequences = find_non_consecutive_sequences(x_test, detection_x_test,delta,column_name=\"recognized_label\")\n",
    "            test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in data_sequences]\n",
    "            test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "            #test\n",
    "            y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "            # save the results in a file\n",
    "            df_temp= pd.DataFrame()\n",
    "            df_temp[\"start\"]=detection_x_test[\"start\"]\n",
    "            df_temp[\"end\"]=detection_x_test[\"end\"]\n",
    "            df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "            if flag_save_results:\n",
    "                folder_path = dir_exp1_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                create_reference_label_file(folder_path+file_name_cycles_DS1[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_exp1 = pd.DataFrame(columns=['cycle_classifier', 'state_classifier', \n",
    "                                        'Abnormal mean F1-score','Abnormal mean Precision','Abnormal mean Recall',\n",
    "                                        'Normal mean F1-score','Normal mean Precision','Normal mean Recall',\n",
    "                                        'Overall mean F1-score','Overall mean Precision','Overall mean Recall',\n",
    "                                        'Abnormal std F1-score','Abnormal std Precision','Abnormal std Recall',\n",
    "                                        'Normal std F1-score','Normal std Precision','Normal std Recall',\n",
    "                                        'Overall std F1-score','Overall std Precision','Overall std Recall'])\n",
    "\n",
    "seeds=list(range(0,10))\n",
    "classifiers=[\"xgboost\",\"rf\",\"dt\",\"nb\",\"xtree\",\"mlp\"]\n",
    "classifiers_w_seed=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "classifiers_wo_seed=[\"xgboost\",\"nb\"]\n",
    "dir = './results/reference_cycle_labels/'\n",
    "reference_path = dir\n",
    "collar = 202.75\n",
    "\n",
    "\n",
    "# 1) classifier_cycle with seed - classifier_state with seed\n",
    "for classifier_cycle in classifiers_w_seed:\n",
    "    for classifier_state in classifiers_w_seed:\n",
    "        detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "        detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "        f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[]\n",
    "        for seed_cycle in seeds:\n",
    "            for seed_state in seeds:\n",
    "                result_path = dir_exp1_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "                f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "                f1score_files_overall.append(f1score_file)\n",
    "                precision_files_overall.append(precision_file)\n",
    "                recall_files_overall.append(recall_file)\n",
    "                f1score_files_abnormal.append(f1score_abnormal)\n",
    "                precision_files_abnormal.append(precision_abnormal)\n",
    "                recall_files_abnormal.append(recall_abnormal)\n",
    "                f1score_files_normal.append(f1score_normal)\n",
    "                precision_files_normal.append(precision_normal)\n",
    "                recall_files_normal.append(recall_normal)\n",
    "\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Abnormal mean Precision':np.mean(precision_files_abnormal)*100,\n",
    "                            'Abnormal mean Recall':np.mean(recall_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Normal mean Precision':np.mean(precision_files_normal)*100,\n",
    "                            'Normal mean Recall':np.mean(recall_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Overall mean Precision':np.mean(precision_files_overall)*100,\n",
    "                            'Overall mean Recall':np.mean(recall_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Abnormal std Precision':np.std(precision_files_abnormal)*100,\n",
    "                            'Abnormal std Recall':np.std(recall_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Normal std Precision':np.std(precision_files_normal)*100,\n",
    "                            'Normal std Recall':np.std(recall_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100,\n",
    "                            'Overall std Precision':np.std(precision_files_overall)*100,\n",
    "                            'Overall std Recall':np.std(recall_files_overall)*100},index=[0])\n",
    "        \n",
    "        df_results_exp1 = pd.concat([df_results_exp1, dflocal], ignore_index=True)\n",
    "\n",
    "# 2) classifier_cycle with seed - classifier_state without seed\n",
    "for classifier_cycle in classifiers_w_seed:\n",
    "    for classifier_state in classifiers_wo_seed:\n",
    "        detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "        detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "        f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[]\n",
    "        for seed_cycle in seeds:\n",
    "            result_path = dir_exp1_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"\n",
    "            f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "            f1score_files_overall.append(f1score_file)\n",
    "            precision_files_overall.append(precision_file)\n",
    "            recall_files_overall.append(recall_file)\n",
    "            f1score_files_abnormal.append(f1score_abnormal)\n",
    "            precision_files_abnormal.append(precision_abnormal)\n",
    "            recall_files_abnormal.append(recall_abnormal)\n",
    "            f1score_files_normal.append(f1score_normal)\n",
    "            precision_files_normal.append(precision_normal)\n",
    "            recall_files_normal.append(recall_normal)\n",
    "\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Abnormal mean Precision':np.mean(precision_files_abnormal)*100,\n",
    "                            'Abnormal mean Recall':np.mean(recall_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Normal mean Precision':np.mean(precision_files_normal)*100,\n",
    "                            'Normal mean Recall':np.mean(recall_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Overall mean Precision':np.mean(precision_files_overall)*100,\n",
    "                            'Overall mean Recall':np.mean(recall_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Abnormal std Precision':np.std(precision_files_abnormal)*100,\n",
    "                            'Abnormal std Recall':np.std(recall_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Normal std Precision':np.std(precision_files_normal)*100,\n",
    "                            'Normal std Recall':np.std(recall_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100,\n",
    "                            'Overall std Precision':np.std(precision_files_overall)*100,\n",
    "                            'Overall std Recall':np.std(recall_files_overall)*100},index=[0])\n",
    "        \n",
    "        df_results_exp1 = pd.concat([df_results_exp1, dflocal], ignore_index=True)\n",
    "\n",
    "# 3) classifier_cycle without seed - classifier_state with seed\n",
    "for classifier_cycle in classifiers_wo_seed:\n",
    "    for classifier_state in classifiers_w_seed:\n",
    "        detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "        f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[]\n",
    "        for seed_state in seeds:\n",
    "            result_path = dir_exp1_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "            f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "            f1score_files_overall.append(f1score_file)\n",
    "            precision_files_overall.append(precision_file)\n",
    "            recall_files_overall.append(recall_file)\n",
    "            f1score_files_abnormal.append(f1score_abnormal)\n",
    "            precision_files_abnormal.append(precision_abnormal)\n",
    "            recall_files_abnormal.append(recall_abnormal)\n",
    "            f1score_files_normal.append(f1score_normal)\n",
    "            precision_files_normal.append(precision_normal)\n",
    "            recall_files_normal.append(recall_normal)\n",
    "\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Abnormal mean Precision':np.mean(precision_files_abnormal)*100,\n",
    "                            'Abnormal mean Recall':np.mean(recall_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Normal mean Precision':np.mean(precision_files_normal)*100,\n",
    "                            'Normal mean Recall':np.mean(recall_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Overall mean Precision':np.mean(precision_files_overall)*100,\n",
    "                            'Overall mean Recall':np.mean(recall_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Abnormal std Precision':np.std(precision_files_abnormal)*100,\n",
    "                            'Abnormal std Recall':np.std(recall_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Normal std Precision':np.std(precision_files_normal)*100,\n",
    "                            'Normal std Recall':np.std(recall_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100,\n",
    "                            'Overall std Precision':np.std(precision_files_overall)*100,\n",
    "                            'Overall std Recall':np.std(recall_files_overall)*100},index=[0])\n",
    "        \n",
    "        df_results_exp1 = pd.concat([df_results_exp1, dflocal], ignore_index=True)\n",
    "\n",
    "# 4) classifier_cycle without seed - classifier_state without seed\n",
    "for classifier_cycle in classifiers_wo_seed:\n",
    "    for classifier_state in classifiers_wo_seed:\n",
    "        result_path = dir_exp1_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"\n",
    "        f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':f1score_abnormal*100,\n",
    "                                'Abnormal mean Precision':precision_abnormal*100,\n",
    "                                'Abnormal mean Recall':recall_abnormal*100,\n",
    "                                'Normal mean F1-score':(f1score_normal)*100,\n",
    "                                'Normal mean Precision':(precision_normal)*100,\n",
    "                                'Normal mean Recall':(recall_normal)*100,\n",
    "                                'Overall mean F1-score':f1score_file*100,\n",
    "                                'Overall mean Precision':precision_file*100,\n",
    "                                'Overall mean Recall':recall_file*100},index=[0])\n",
    "        \n",
    "        df_results_exp1 = pd.concat([df_results_exp1, dflocal], ignore_index=True)\n",
    "\n",
    "df_results_exp1.to_csv(dir_exp1_approach3 + 'experiment1_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "Train in DS1 and test in DS2\n",
    "\n",
    "I need to train the duty-cycle classifier using the same process for training the state classifier, i.e. using the four mounts of data.\n",
    "For testing, I will use the state_labels, previously classified.\n",
    "Therefore, it will be multiple combinations of duty-cycle classification with state classification. Each duty-cycle classifier will be tested in all state classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_ref_state= pd.concat(data_DS1)\n",
    "dfs_ref_cycle = pd.concat(timestamps_cycle_DS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Since the results path depend on if the classifier use or not the seed parameters, there are 4 combination of classifiers: \"classifiers_state\" w/ and w/o seed and \"classifier_cycle\" w/ and w/o seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) classifier_cycle with seed - classifier_state with seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "for seed_state in seeds:\n",
    "    for classifier_state in classifiers:\n",
    "        folder_path = dir_exp1_approach2+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "        aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "        dfs_ref_state[\"recog_label\"]=np.concatenate([recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22])\n",
    "\n",
    "        train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"recog_label\")\n",
    "\n",
    "        # # Determine the maximum sequence length for padding and add some extra value\n",
    "        max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "        # # Encode the sequences for training\n",
    "        train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "        X_res = pad_sequences(train_features, max_sequence_length)\n",
    "        y_res = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "\n",
    "        #import classified state_labels\n",
    "        folder_path = dir_exp2_approach2+classifier_state+\"/\"+str(seed_state)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "\n",
    "        #compute the detection of duty-cycle for each month\n",
    "        data_csv_jun23['detected_cycles']=data_csv_jun23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "        data_csv_aug23['detected_cycles']=data_csv_aug23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "        data_csv_okt23['detected_cycles']=data_csv_okt23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "        data_csv_dec23['detected_cycles']=data_csv_dec23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "\n",
    "        detection_jun23 = boundaries_cycles(data_csv_jun23)\n",
    "        detection_aug23 = boundaries_cycles(data_csv_aug23)\n",
    "        detection_okt23 = boundaries_cycles(data_csv_okt23)\n",
    "        detection_dec23 = boundaries_cycles(data_csv_dec23)\n",
    "\n",
    "\n",
    "        #merge test data\n",
    "        dfs_data = [data_csv_jun23,data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "        dfs_detection_cycle = [detection_jun23,detection_aug23,detection_okt23,detection_dec23]\n",
    "\n",
    "        file_name= [\"jun23_cycle.txt\" ,\"aug23_cycle.txt\",\"okt23_cycle.txt\",\"dec23_cycle.txt\"]\n",
    "\n",
    "\n",
    "        for classifier_cycle in classifiers:\n",
    "            for seed_cycle in seeds:\n",
    "                #train\n",
    "                clf_duty = train_cycle_classifier(classifier_cycle,X_res, y_res,seed_cycle)\n",
    "                \n",
    "                #compute the sequence for each month of data\n",
    "                all_sequences = []\n",
    "                for i in range(len(dfs_data)):\n",
    "                    data_sequences = find_non_consecutive_sequences(dfs_data[i], dfs_detection_cycle[i],delta,column_name=\"recognized_label\")\n",
    "                    all_sequences.append(data_sequences)\n",
    "\n",
    "                for i in range(len(dfs_data)):\n",
    "                    test_sequences = all_sequences[i]\n",
    "\n",
    "                    # Encode the sequences for testing\n",
    "                    test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in test_sequences]\n",
    "                    test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "                #test the duty-cycle classifier over the labels classifier with the \n",
    "                # different classifiers evaluated for recognize states\n",
    "\n",
    "                    #test\n",
    "                    y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "                    # save the results in a file\n",
    "                    df_temp= pd.DataFrame()\n",
    "                    df_temp[\"start\"]=dfs_detection_cycle[i][\"start\"]\n",
    "                    df_temp[\"end\"]=dfs_detection_cycle[i][\"end\"]\n",
    "                    df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "                    if flag_save_results:\n",
    "                        folder_path = dir_exp2_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "                        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                        create_reference_label_file(folder_path+file_name[i],df_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) classifier_cycle with seed - classifier_state without seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers_cycle=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "classifiers_state=[\"xgboost\",\"nb\"]\n",
    "\n",
    "for classifier_state in classifiers_state:\n",
    "    folder_path = dir_exp1_approach2+classifier_state+\"/\"\n",
    "    aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "    dfs_ref_state[\"recog_label\"]=np.concatenate([recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22])\n",
    "\n",
    "    train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"recog_label\")\n",
    "\n",
    "    # # Determine the maximum sequence length for padding and add some extra value\n",
    "    max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "    # # Encode the sequences for training\n",
    "    train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "    X_res = pad_sequences(train_features, max_sequence_length)\n",
    "    y_res = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "\n",
    "    #import classified state_labels\n",
    "    folder_path = dir_exp2_approach2+classifier_state\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "\n",
    "    #compute the detection of duty-cycle for each month\n",
    "    data_csv_jun23['detected_cycles']=data_csv_jun23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    data_csv_aug23['detected_cycles']=data_csv_aug23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    data_csv_okt23['detected_cycles']=data_csv_okt23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    data_csv_dec23['detected_cycles']=data_csv_dec23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "\n",
    "    detection_jun23 = boundaries_cycles(data_csv_jun23)\n",
    "    detection_aug23 = boundaries_cycles(data_csv_aug23)\n",
    "    detection_okt23 = boundaries_cycles(data_csv_okt23)\n",
    "    detection_dec23 = boundaries_cycles(data_csv_dec23)\n",
    "\n",
    "\n",
    "    #merge test data\n",
    "    dfs_data = [data_csv_jun23,data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "    dfs_detection_cycle = [detection_jun23,detection_aug23,detection_okt23,detection_dec23]\n",
    "\n",
    "    file_name= [\"jun23_cycle.txt\" ,\"aug23_cycle.txt\",\"okt23_cycle.txt\",\"dec23_cycle.txt\"]\n",
    "\n",
    "\n",
    "    for seed_cycle in seeds:\n",
    "        for classifier_cycle in classifiers_cycle:\n",
    "            #train\n",
    "            clf_duty = train_cycle_classifier(classifier_cycle,X_res, y_res,seed_cycle)\n",
    "            \n",
    "            #test the duty-cycle classifier over the labels classifier with the \n",
    "            # different classifiers evaluated for recognize states\n",
    "        \n",
    "\n",
    "            #compute the sequence for each month of data\n",
    "            all_sequences = []\n",
    "            for i in range(len(dfs_data)):\n",
    "                data_sequences = find_non_consecutive_sequences(dfs_data[i], dfs_detection_cycle[i],delta,column_name=\"recognized_label\")\n",
    "                all_sequences.append(data_sequences)\n",
    "\n",
    "            for i in range(len(dfs_data)):\n",
    "                test_sequences = all_sequences[i]\n",
    "\n",
    "                # Encode the sequences for testing\n",
    "                test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in test_sequences]\n",
    "                test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "                #test\n",
    "                y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "                # save the results in a file\n",
    "                df_temp= pd.DataFrame()\n",
    "                df_temp[\"start\"]=dfs_detection_cycle[i][\"start\"]\n",
    "                df_temp[\"end\"]=dfs_detection_cycle[i][\"end\"]\n",
    "                df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "                if flag_save_results:\n",
    "                    folder_path = dir_exp2_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                    create_reference_label_file(folder_path+file_name[i],df_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) classifier_cycle without seed - classifier_state with seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers_cycle=[\"nb\",\"xgboost\"]\n",
    "classifiers_state=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "for seed in seeds:\n",
    "    for classifier_state in classifiers_state:\n",
    "        folder_path = dir_exp1_approach2+classifier_state+\"/\"+str(seed)+\"/\"\n",
    "        aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "        dfs_ref_state[\"recog_label\"]=np.concatenate([recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22])\n",
    "\n",
    "        train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"recog_label\")\n",
    "\n",
    "        # # Determine the maximum sequence length for padding and add some extra value\n",
    "        max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "        # # Encode the sequences for training\n",
    "        train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "        X_res = pad_sequences(train_features, max_sequence_length)\n",
    "        y_res = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "\n",
    "        #import classified state_labels\n",
    "        folder_path = dir_exp2_approach2+classifier_state+\"/\"+str(seed)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "\n",
    "        #compute the detection of duty-cycle for each month\n",
    "        data_csv_jun23['detected_cycles']=data_csv_jun23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "        data_csv_aug23['detected_cycles']=data_csv_aug23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "        data_csv_okt23['detected_cycles']=data_csv_okt23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "        data_csv_dec23['detected_cycles']=data_csv_dec23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "\n",
    "        detection_jun23 = boundaries_cycles(data_csv_jun23)\n",
    "        detection_aug23 = boundaries_cycles(data_csv_aug23)\n",
    "        detection_okt23 = boundaries_cycles(data_csv_okt23)\n",
    "        detection_dec23 = boundaries_cycles(data_csv_dec23)\n",
    "\n",
    "\n",
    "        #merge test data\n",
    "        dfs_data = [data_csv_jun23,data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "        dfs_detection_cycle = [detection_jun23,detection_aug23,detection_okt23,detection_dec23]\n",
    "\n",
    "        file_name= [\"jun23_cycle.txt\" ,\"aug23_cycle.txt\",\"okt23_cycle.txt\",\"dec23_cycle.txt\"]\n",
    "\n",
    "        for classifier_cycle in classifiers_cycle:\n",
    "            \n",
    "            #train\n",
    "            clf_duty = train_cycle_classifier(classifier_cycle,X_res, y_res)\n",
    "    \n",
    "    #test the duty-cycle classifier over the labels classifier with the \n",
    "    # different classifiers evaluated for recognize states            \n",
    "\n",
    "            #compute the sequence for each month of data\n",
    "            all_sequences = []\n",
    "            for i in range(len(dfs_data)):\n",
    "                data_sequences = find_non_consecutive_sequences(dfs_data[i], dfs_detection_cycle[i],delta,column_name=\"recognized_label\")\n",
    "                all_sequences.append(data_sequences)\n",
    "\n",
    "            for i in range(len(dfs_data)):\n",
    "                test_sequences = all_sequences[i]\n",
    "\n",
    "                # Encode the sequences for testing\n",
    "                test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in test_sequences]\n",
    "                test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "                #test\n",
    "                y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "                # save the results in a file\n",
    "                df_temp= pd.DataFrame()\n",
    "                df_temp[\"start\"]=dfs_detection_cycle[i][\"start\"]\n",
    "                df_temp[\"end\"]=dfs_detection_cycle[i][\"end\"]\n",
    "                df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "                if flag_save_results:\n",
    "                    folder_path = dir_exp2_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"+str(seed)+\"/\"\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                    create_reference_label_file(folder_path+file_name[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) classifier_cycle without seed - classifier_state without seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "for classifier_state in classifiers:\n",
    "    folder_path = dir_exp1_approach2+classifier_state+\"/\"\n",
    "    aux= df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "    dfs_ref_state[\"recog_label\"]=np.concatenate([recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22])\n",
    "\n",
    "    train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"recog_label\")\n",
    "\n",
    "    # # Determine the maximum sequence length for padding and add some extra value\n",
    "    max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "    # # Encode the sequences for training\n",
    "    train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "    X_res = pad_sequences(train_features, max_sequence_length)\n",
    "    y_res = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "\n",
    "#import classified state_labels\n",
    "    folder_path = dir_exp2_approach2+classifier_state\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "\n",
    "    #compute the detection of duty-cycle for each month\n",
    "    data_csv_jun23['detected_cycles']=data_csv_jun23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    data_csv_aug23['detected_cycles']=data_csv_aug23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    data_csv_okt23['detected_cycles']=data_csv_okt23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "    data_csv_dec23['detected_cycles']=data_csv_dec23[\"Speed_order3\"].apply(lambda x: detect_cycle(x))\n",
    "\n",
    "    detection_jun23 = boundaries_cycles(data_csv_jun23)\n",
    "    detection_aug23 = boundaries_cycles(data_csv_aug23)\n",
    "    detection_okt23 = boundaries_cycles(data_csv_okt23)\n",
    "    detection_dec23 = boundaries_cycles(data_csv_dec23)\n",
    "\n",
    "\n",
    "    #merge test data\n",
    "    dfs_data = [data_csv_jun23,data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "    dfs_detection_cycle = [detection_jun23,detection_aug23,detection_okt23,detection_dec23]\n",
    "\n",
    "    file_name= [\"jun23_cycle.txt\" ,\"aug23_cycle.txt\",\"okt23_cycle.txt\",\"dec23_cycle.txt\"]\n",
    "\n",
    "\n",
    "    for classifier_cycle in classifiers:\n",
    "        #train\n",
    "        clf_duty = train_cycle_classifier(classifier_cycle,X_res, y_res)\n",
    "    \n",
    "    #test the duty-cycle classifier over the labels classifier with the \n",
    "    # different classifiers evaluated for recognize states\n",
    "\n",
    "        #compute the sequence for each month of data\n",
    "        all_sequences = []\n",
    "        for i in range(len(dfs_data)):\n",
    "            data_sequences = find_non_consecutive_sequences(dfs_data[i], dfs_detection_cycle[i],delta,column_name=\"recognized_label\")\n",
    "            all_sequences.append(data_sequences)\n",
    "\n",
    "        for i in range(len(dfs_data)):\n",
    "            test_sequences = all_sequences[i]\n",
    "\n",
    "            # Encode the sequences for testing\n",
    "            test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in test_sequences]\n",
    "            test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "            # # Normalize features\n",
    "            # test_features_padded = scaler_cycle.fit_transform(test_features_padded)\n",
    "\n",
    "            #test\n",
    "            y_pred = clf_duty.predict(test_features_padded)\n",
    "\n",
    "            # save the results in a file\n",
    "            df_temp= pd.DataFrame()\n",
    "            df_temp[\"start\"]=dfs_detection_cycle[i][\"start\"]\n",
    "            df_temp[\"end\"]=dfs_detection_cycle[i][\"end\"]\n",
    "            df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "            if flag_save_results:\n",
    "                folder_path = dir_exp2_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                create_reference_label_file(folder_path+file_name[i],df_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_exp2 = pd.DataFrame(columns=['cycle_classifier', 'state_classifier', \n",
    "                                        'Abnormal mean F1-score','Abnormal mean Precision','Abnormal mean Recall',\n",
    "                                        'Normal mean F1-score','Normal mean Precision','Normal mean Recall',\n",
    "                                        'Overall mean F1-score','Overall mean Precision','Overall mean Recall',\n",
    "                                        'Abnormal std F1-score','Abnormal std Precision','Abnormal std Recall',\n",
    "                                        'Normal std F1-score','Normal std Precision','Normal std Recall',\n",
    "                                        'Overall std F1-score','Overall std Precision','Overall std Recall'])\n",
    "\n",
    "seeds=list(range(0,10))\n",
    "classifiers=[\"xgboost\",\"rf\",\"dt\",\"nb\",\"xtree\",\"mlp\"]\n",
    "classifiers_w_seed=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "classifiers_wo_seed=[\"xgboost\",\"nb\"]\n",
    "dir = './results/reference_cycle_labels/'\n",
    "reference_path = dir\n",
    "collar = 202.75\n",
    "\n",
    "\n",
    "# 1) classifier_cycle with seed - classifier_state with seed\n",
    "for classifier_cycle in classifiers_w_seed:\n",
    "    for classifier_state in classifiers_w_seed:\n",
    "        detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "        detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "        f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[]\n",
    "        for seed_cycle in seeds:\n",
    "            for seed_state in seeds:\n",
    "                result_path = dir_exp2_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "                f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "                check_nan=(f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal)\n",
    "                if all(check_nan):\n",
    "                    f1score_files_overall.append(f1score_file)\n",
    "                    precision_files_overall.append(precision_file)\n",
    "                    recall_files_overall.append(recall_file)\n",
    "                    f1score_files_abnormal.append(f1score_abnormal)\n",
    "                    precision_files_abnormal.append(precision_abnormal)\n",
    "                    recall_files_abnormal.append(recall_abnormal)\n",
    "                    f1score_files_normal.append(f1score_normal)\n",
    "                    precision_files_normal.append(precision_normal)\n",
    "                    recall_files_normal.append(recall_normal)\n",
    "\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Abnormal mean Precision':np.mean(precision_files_abnormal)*100,\n",
    "                            'Abnormal mean Recall':np.mean(recall_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Normal mean Precision':np.mean(precision_files_normal)*100,\n",
    "                            'Normal mean Recall':np.mean(recall_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Overall mean Precision':np.mean(precision_files_overall)*100,\n",
    "                            'Overall mean Recall':np.mean(recall_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Abnormal std Precision':np.std(precision_files_abnormal)*100,\n",
    "                            'Abnormal std Recall':np.std(recall_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Normal std Precision':np.std(precision_files_normal)*100,\n",
    "                            'Normal std Recall':np.std(recall_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100,\n",
    "                            'Overall std Precision':np.std(precision_files_overall)*100,\n",
    "                            'Overall std Recall':np.std(recall_files_overall)*100},index=[0])\n",
    "        \n",
    "        df_results_exp2 = pd.concat([df_results_exp2, dflocal], ignore_index=True)\n",
    "\n",
    "# 2) classifier_cycle with seed - classifier_state without seed\n",
    "for classifier_cycle in classifiers_w_seed:\n",
    "    for classifier_state in classifiers_wo_seed:\n",
    "        detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "        detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "        f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[]\n",
    "        for seed_cycle in seeds:\n",
    "            result_path = dir_exp2_approach3+classifier_cycle+\"/\"+str(seed_cycle)+\"/\"+classifier_state+\"/\"\n",
    "            f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "            check_nan=(f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal)\n",
    "            if all(check_nan):\n",
    "                f1score_files_overall.append(f1score_file)\n",
    "                precision_files_overall.append(precision_file)\n",
    "                recall_files_overall.append(recall_file)\n",
    "                f1score_files_abnormal.append(f1score_abnormal)\n",
    "                precision_files_abnormal.append(precision_abnormal)\n",
    "                recall_files_abnormal.append(recall_abnormal)\n",
    "                f1score_files_normal.append(f1score_normal)\n",
    "                precision_files_normal.append(precision_normal)\n",
    "                recall_files_normal.append(recall_normal)\n",
    "\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Abnormal mean Precision':np.mean(precision_files_abnormal)*100,\n",
    "                            'Abnormal mean Recall':np.mean(recall_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Normal mean Precision':np.mean(precision_files_normal)*100,\n",
    "                            'Normal mean Recall':np.mean(recall_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Overall mean Precision':np.mean(precision_files_overall)*100,\n",
    "                            'Overall mean Recall':np.mean(recall_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Abnormal std Precision':np.std(precision_files_abnormal)*100,\n",
    "                            'Abnormal std Recall':np.std(recall_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Normal std Precision':np.std(precision_files_normal)*100,\n",
    "                            'Normal std Recall':np.std(recall_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100,\n",
    "                            'Overall std Precision':np.std(precision_files_overall)*100,\n",
    "                            'Overall std Recall':np.std(recall_files_overall)*100},index=[0])\n",
    "        \n",
    "        df_results_exp2 = pd.concat([df_results_exp2, dflocal], ignore_index=True)\n",
    "\n",
    "# 3) classifier_cycle without seed - classifier_state with seed\n",
    "for classifier_cycle in classifiers_wo_seed:\n",
    "    for classifier_state in classifiers_w_seed:\n",
    "        detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "        f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[]\n",
    "        for seed_state in seeds:\n",
    "            result_path = dir_exp2_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"+str(seed_state)+\"/\"\n",
    "            f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "            check_nan=(f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal)\n",
    "            if all(check_nan):\n",
    "                f1score_files_overall.append(f1score_file)\n",
    "                precision_files_overall.append(precision_file)\n",
    "                recall_files_overall.append(recall_file)\n",
    "                f1score_files_abnormal.append(f1score_abnormal)\n",
    "                precision_files_abnormal.append(precision_abnormal)\n",
    "                recall_files_abnormal.append(recall_abnormal)\n",
    "                f1score_files_normal.append(f1score_normal)\n",
    "                precision_files_normal.append(precision_normal)\n",
    "                recall_files_normal.append(recall_normal)\n",
    "\n",
    "        dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Abnormal mean Precision':np.mean(precision_files_abnormal)*100,\n",
    "                            'Abnormal mean Recall':np.mean(recall_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Normal mean Precision':np.mean(precision_files_normal)*100,\n",
    "                            'Normal mean Recall':np.mean(recall_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Overall mean Precision':np.mean(precision_files_overall)*100,\n",
    "                            'Overall mean Recall':np.mean(recall_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Abnormal std Precision':np.std(precision_files_abnormal)*100,\n",
    "                            'Abnormal std Recall':np.std(recall_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Normal std Precision':np.std(precision_files_normal)*100,\n",
    "                            'Normal std Recall':np.std(recall_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100,\n",
    "                            'Overall std Precision':np.std(precision_files_overall)*100,\n",
    "                            'Overall std Recall':np.std(recall_files_overall)*100},index=[0])\n",
    "        \n",
    "        df_results_exp2 = pd.concat([df_results_exp2, dflocal], ignore_index=True)\n",
    "\n",
    "# 4) classifier_cycle without seed - classifier_state without seed\n",
    "for classifier_cycle in classifiers_wo_seed:\n",
    "    for classifier_state in classifiers_wo_seed:\n",
    "        result_path = dir_exp2_approach3+classifier_cycle+\"/\"+classifier_state+\"/\"\n",
    "        f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal = compute_classification_sedeval(reference_path,result_path,collar)\n",
    "        check_nan=(f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal, f1score_normal, precision_normal, recall_normal)\n",
    "        if all(check_nan):\n",
    "            dflocal = pd.DataFrame({'cycle_classifier':classifier_cycle, 'state_classifier':classifier_state, \n",
    "                                'Abnormal mean F1-score':f1score_abnormal*100,\n",
    "                                'Abnormal mean Precision':precision_abnormal*100,\n",
    "                                'Abnormal mean Recall':recall_abnormal*100,\n",
    "                                'Normal mean F1-score':(f1score_normal)*100,\n",
    "                                'Normal mean Precision':(precision_normal)*100,\n",
    "                                'Normal mean Recall':(recall_normal)*100,\n",
    "                                'Overall mean F1-score':f1score_file*100,\n",
    "                                'Overall mean Precision':precision_file*100,\n",
    "                                'Overall mean Recall':recall_file*100},index=[0])\n",
    "        \n",
    "            df_results_exp2 = pd.concat([df_results_exp2, dflocal], ignore_index=True)\n",
    "\n",
    "df_results_exp2.to_csv(dir_exp2_approach3+'experiment2_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment on MCU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset= pd.concat([data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22])\n",
    "\n",
    "df_dataset[\"ref_label\"]= np.concatenate((true_label_jun21, true_label_okt21,true_label_jan22,true_label_april22), axis=0)    \n",
    "df_dataset[\"ref_label_cycle\"]= np.concatenate((true_label_cycle_jun21, true_label_cycle_okt21,true_label_cycle_jan22,true_label_cycle_april22), axis=0)    \n",
    "\n",
    "removed_indices = df_dataset[df_dataset['ref_label'].isnull()].index.tolist()\n",
    "df_dataset = df_dataset[df_dataset['ref_label'].notnull()]\n",
    "df_dataset=df_dataset.reset_index()\n",
    "\n",
    "# remove the recognized_label column added in the experiment1 or experiment2\n",
    "if 'recognized_label' in df_dataset.columns:\n",
    "    df_dataset = df_dataset.drop('recognized_label', axis=1)\n",
    "\n",
    "x = df_dataset[df_dataset.columns[1:-2]]\n",
    "y_cycle = df_dataset[df_dataset.columns[-1]]\n",
    "y_state = df_dataset[df_dataset.columns[-2]]\n",
    "\n",
    "y_state [y_state=='E']='B'\n",
    "\n",
    "# normalize feature to range [0;1]\n",
    "scaler = MinMaxScaler(clip=True)\n",
    "scaler.fit(x)\n",
    "x_train = pd.DataFrame(scaler.transform(x), columns=x.columns)\n",
    "\n",
    "y_state [y_state=='E']='B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance ds1 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_balanced, y_train_balanced,_ = balance_dataset(x_train,y_state)\n",
    "\n",
    "# Print balanced dataset\n",
    "unique_values, counts = np.unique(y_state, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "print(\"Value class-counts in Unbalanced dataset:\",value_counts)\n",
    "print(\"Value class-porcentage in Unbalanced dataset:\",value_porcentages)\n",
    "\n",
    "unique_values, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "print(\"Value class-counts in Balanced dataset:\",value_counts)\n",
    "print(\"Value class-porcentage in Unbalanced dataset:\",value_porcentages)\n",
    "\n",
    "del unique_values,counts,value_counts,value_porcentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SCALED VALUES TO BE ADDED IN THE C-CODE\")\n",
    "print(f\"Minimun: \",scaler.data_min_)\n",
    "print(f\"Maximun: \",scaler.data_max_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data (DS2)\n",
    "df_testset= pd.concat([data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23])\n",
    "x_test_float = pd.DataFrame(scaler.transform(df_testset), columns=df_testset.columns,index=df_testset.index)\n",
    "\n",
    "x_train_float= x_train_balanced.copy()\n",
    "x_train_float.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_ref_state= pd.concat(data_DS1)\n",
    "dfs_ref_cycle = pd.concat(timestamps_cycle_DS1)\n",
    "\n",
    "data_csv_jun23_float=pd.DataFrame(scaler.transform(data_csv_jun23), columns=data_csv_jun23.columns,index=data_csv_jun23.index)\n",
    "data_csv_aug23_float=pd.DataFrame(scaler.transform(data_csv_aug23), columns=data_csv_aug23.columns,index=data_csv_aug23.index)\n",
    "data_csv_okt23_float=pd.DataFrame(scaler.transform(data_csv_okt23), columns=data_csv_okt23.columns,index=data_csv_okt23.index)\n",
    "data_csv_dec23_float=pd.DataFrame(scaler.transform(data_csv_dec23), columns=data_csv_dec23.columns,index=data_csv_dec23.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp + xtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import layers,metrics\n",
    "print(tf.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' \n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_state=\"mlp\"\n",
    "classifier_cycle=\"xtree\"\n",
    "seed=0\n",
    "\n",
    "dir_model_pred = \"./results/approach3/MCU/\"\n",
    "model_path = \"./results/c_code/approach3/\"\n",
    "TF_MODEL = \"mlp_float.keras\"\n",
    "TF_MODEL_I8=\"mlp_int8.tflite\"\n",
    "\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_state_float = train_state_supervised_classifier(classifier_state,x_train_float, y_train_balanced,seed)\n",
    "clf_state_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce in TensorFlow the MLP obtained with Sklearn\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(12, input_dim=len(x_train_float.columns), activation='relu')) \n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer,loss='SparseCategoricalCrossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS=50\n",
    "history = model.fit(x=x_train_float, y=y_train_balanced,epochs=NUM_EPOCHS,shuffle=True)\n",
    "model.save(model_path+TF_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = tf.keras.models.load_model(model_path+TF_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion without quantization\n",
    "TFL_MODEL_FILE= model_path +\"mlp_float.tflite\"\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_load)\n",
    "tflite_model = converter.convert()\n",
    "open(TFL_MODEL_FILE, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=TFL_MODEL_FILE)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "\n",
    "x_test_float32 = x_test_float.to_numpy().astype(\"float32\")\n",
    "\n",
    "for x in x_test_float32:\n",
    "  interpreter.set_tensor(input_details[0]['index'], x.reshape(input_details[0]['shape']))\n",
    "  interpreter.invoke()\n",
    "  output_data.append(interpreter.get_tensor(output_details[0]['index'])[0])\n",
    "\n",
    "y_test_pred_lite = np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_details[0])\n",
    "print(output_details[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test state-cycles\n",
    "y_predict = np.argmax(y_test_pred_lite, axis=1)\n",
    "y_pred_smoothed = smooth_labels(y_predict,3)     #apply 3rd median filter\n",
    "y_recognized=le_state.inverse_transform(y_pred_smoothed.astype(int))\n",
    "df_temp=df_testset.copy()\n",
    "df_temp[\"recognized_label\"]=y_recognized\n",
    "df_temp=df_temp.reset_index()\n",
    "\n",
    "#classify duty-cycle\n",
    "df_recognized_states_jun23 = create_segments_state(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_temp)\n",
    "df_recognized_states_aug23 = create_segments_state(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_temp)\n",
    "df_recognized_states_okt23 = create_segments_state(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_temp)\n",
    "df_recognized_states_dec23 = create_segments_state(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_temp)\n",
    "\n",
    "folder_path = dir_model_pred+\"/float\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "if flag_save_results:\n",
    "    create_reference_label_file(folder_path+\"/jun23_state.txt\",df_recognized_states_jun23)\n",
    "    create_reference_label_file(folder_path+\"/aug23_state.txt\",df_recognized_states_aug23)\n",
    "    create_reference_label_file(folder_path+\"/okt23_state.txt\",df_recognized_states_okt23)\n",
    "    create_reference_label_file(folder_path+\"/dec23_state.txt\",df_recognized_states_dec23)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_train_labels = dir_exp1_approach2+classifier_state+\"/\"+str(seed)+\"/\"\n",
    "aux= df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "dfs_ref_state[\"recog_label\"]=np.concatenate([recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22])\n",
    "\n",
    "train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"recog_label\")\n",
    "\n",
    "# # Determine the maximum sequence length for padding and add some extra value\n",
    "max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "# # Encode the sequences for training\n",
    "train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "train_features_padded = pad_sequences(train_features, max_sequence_length)\n",
    "train_labels = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "X_res, y_res = train_features_padded, train_labels\n",
    "\n",
    "#import classified state_labels\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_jun23_float[\"recognized_label\"] = ndarray_labels(data_csv_jun23_float.index[0],data_csv_jun23_float.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_aug23_float[\"recognized_label\"] = ndarray_labels(data_csv_aug23_float.index[0],data_csv_aug23_float.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_okt23_float[\"recognized_label\"] = ndarray_labels(data_csv_okt23_float.index[0],data_csv_okt23_float.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_dec23_float[\"recognized_label\"] = ndarray_labels(data_csv_dec23_float.index[0],data_csv_dec23_float.index[-1],aux,downsampled_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the detection of duty-cycle for each month\n",
    "threshold_speed = 2.5\n",
    "threshold_speed_normalized = (threshold_speed-scaler.data_min_[2])/scaler.data_range_[2]\n",
    "# threshold_speed_quant = (threshold_speed_normalized*(2**32 - 1)).astype('float')\n",
    "data_csv_jun23_float.loc[data_csv_jun23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "data_csv_aug23_float.loc[data_csv_aug23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "data_csv_okt23_float.loc[data_csv_okt23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "data_csv_dec23_float.loc[data_csv_dec23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "\n",
    "detection_jun23 = boundaries_cycles(data_csv_jun23_float)\n",
    "detection_aug23 = boundaries_cycles(data_csv_aug23_float)\n",
    "detection_okt23 = boundaries_cycles(data_csv_okt23_float)\n",
    "detection_dec23 = boundaries_cycles(data_csv_dec23_float)\n",
    "\n",
    "#merge test data\n",
    "dfs_data = [data_csv_jun23_float,data_csv_aug23_float,data_csv_okt23_float,data_csv_dec23_float]\n",
    "dfs_detection_cycle = [detection_jun23,detection_aug23,detection_okt23,detection_dec23]\n",
    "\n",
    "file_name= [\"jun23_cycle.txt\" ,\"aug23_cycle.txt\", \n",
    "            \"okt23_cycle.txt\",\"dec23_cycle.txt\"]\n",
    "\n",
    "#train\n",
    "clf_duty_float = train_cycle_classifier(classifier_cycle,X_res, y_res,seed)\n",
    "\n",
    "#compute the sequence for each month of data\n",
    "all_sequences = []\n",
    "for i in range(len(dfs_data)):\n",
    "    data_sequences = find_non_consecutive_sequences(dfs_data[i], dfs_detection_cycle[i],delta,column_name=\"recognized_label\")\n",
    "    all_sequences.append(data_sequences)\n",
    "\n",
    "for i in range(len(dfs_data)):\n",
    "    test_sequences = all_sequences[i]\n",
    "\n",
    "    # Encode the sequences for testing\n",
    "    test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in test_sequences]\n",
    "    test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "    #test\n",
    "    y_pred = clf_duty_float.predict(test_features_padded)\n",
    "\n",
    "    # save the results in a file\n",
    "    df_temp= pd.DataFrame()\n",
    "    df_temp[\"start\"]=dfs_detection_cycle[i][\"start\"]\n",
    "    df_temp[\"end\"]=dfs_detection_cycle[i][\"end\"]\n",
    "    df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "    if flag_save_results:\n",
    "        create_reference_label_file(folder_path+\"/\"+file_name[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert model to C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i ./results/c_code/approach3/mlp_float.tflite > ./results/c_code/approach3/mlp_float.h\n",
    "!cat ./results/c_code/approach3/mlp_float.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlearn import convert\n",
    "\n",
    "cmodel = convert(clf_duty_float, method='inline',dtype='float')\n",
    "cmodel.save(file = model_path+\"clf_duty_float.h\", name='clf_duty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "  for i_value in tf.data.Dataset.from_tensor_slices(x_test_float).batch(1).take(int(len(x_test_float)/2)):\n",
    "    i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
    "    yield [i_value_f32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_load)\n",
    "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_int_quant = converter.convert()\n",
    "TF_MODEL_I8= model_path + \"mlp_int8.tflite\"\n",
    "open(TF_MODEL_I8, \"wb\").write(tflite_model_int_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_I8)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Here we manually quantize the float32 data to provide int8 inputs\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "np_x_test = x_test_float32 / input_scale + input_zero_point\n",
    "\n",
    "output_data = []\n",
    "\n",
    "for x in np_x_test:\n",
    "  interpreter.set_tensor(input_details[0]['index'], x.reshape(input_details[0]['shape']).astype(\"int8\"))\n",
    "  interpreter.invoke()\n",
    "  output_data.append(interpreter.get_tensor(output_details[0]['index'])[0])\n",
    "\n",
    "y_test_pred_lite_iquant = np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_details[0])\n",
    "print(output_details[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test state-cycles\n",
    "y_predict = np.argmax(y_test_pred_lite_iquant, axis=1)\n",
    "y_pred_smoothed = smooth_labels(y_predict,3)     #apply 3rd median filter\n",
    "y_recognized=le_state.inverse_transform(y_pred_smoothed.astype(int))\n",
    "df_temp=df_testset.copy()\n",
    "df_temp[\"recognized_label\"]=y_recognized\n",
    "df_temp=df_temp.reset_index()\n",
    "\n",
    "#classify duty-cycle\n",
    "df_recognized_states_jun23 = create_segments_state(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_temp)\n",
    "df_recognized_states_aug23 = create_segments_state(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_temp)\n",
    "df_recognized_states_okt23 = create_segments_state(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_temp)\n",
    "df_recognized_states_dec23 = create_segments_state(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_temp)\n",
    "\n",
    "folder_path = dir_model_pred+\"int8\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "if True:\n",
    "    create_reference_label_file(folder_path+\"/jun23_state.txt\",df_recognized_states_jun23)\n",
    "    create_reference_label_file(folder_path+\"/aug23_state.txt\",df_recognized_states_aug23)\n",
    "    create_reference_label_file(folder_path+\"/okt23_state.txt\",df_recognized_states_okt23)\n",
    "    create_reference_label_file(folder_path+\"/dec23_state.txt\",df_recognized_states_dec23)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_train_labels = dir_exp1_approach2+classifier_state+\"/\"+str(seed)+\"/\"\n",
    "aux= df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[0],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_jun21 = ndarray_labels(data_DS1[0].index[0],data_DS1[0].index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[1],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_okt21 = ndarray_labels(data_DS1[1].index[0],data_DS1[1].index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[2],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_jan22 = ndarray_labels(data_DS1[2].index[0],data_DS1[2].index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path_train_labels+file_name_states_DS1[3],sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "recog_label_april22 = ndarray_labels(data_DS1[3].index[0],data_DS1[3].index[-1],aux,downsampled_freq)\n",
    "dfs_ref_state[\"recog_label\"]=np.concatenate([recog_label_jun21, recog_label_okt21,recog_label_jan22,recog_label_april22])\n",
    "\n",
    "train_sequences = find_non_consecutive_sequences(dfs_ref_state, dfs_ref_cycle,delta,column_name=\"recog_label\")\n",
    "\n",
    "# # Determine the maximum sequence length for padding and add some extra value\n",
    "max_sequence_length = max(len(seq['non_consecutive_labels']) for seq in train_sequences)+3\n",
    "\n",
    "# # Encode the sequences for training\n",
    "train_features = [le_state.transform(seq['non_consecutive_labels']) for seq in train_sequences]\n",
    "train_features_padded = pad_sequences(train_features, max_sequence_length)\n",
    "train_labels = le_cycle.transform([seq['label'] for seq in train_sequences])\n",
    "X_res, y_res = train_features_padded, train_labels\n",
    "\n",
    "#import classified state_labels\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_jun23_float[\"recognized_label\"] = ndarray_labels(data_csv_jun23_float.index[0],data_csv_jun23_float.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_aug23_float[\"recognized_label\"] = ndarray_labels(data_csv_aug23_float.index[0],data_csv_aug23_float.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_okt23_float[\"recognized_label\"] = ndarray_labels(data_csv_okt23_float.index[0],data_csv_okt23_float.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_dec23_float[\"recognized_label\"] = ndarray_labels(data_csv_dec23_float.index[0],data_csv_dec23_float.index[-1],aux,downsampled_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the detection of duty-cycle for each month\n",
    "threshold_speed = 2.5\n",
    "threshold_speed_normalized = (threshold_speed-scaler.data_min_[2])/scaler.data_range_[2]\n",
    "# threshold_speed_quant = (threshold_speed_normalized*(2**32 - 1)).astype('float')\n",
    "data_csv_jun23_float.loc[data_csv_jun23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "data_csv_aug23_float.loc[data_csv_aug23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "data_csv_okt23_float.loc[data_csv_okt23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "data_csv_dec23_float.loc[data_csv_dec23_float['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "\n",
    "detection_jun23 = boundaries_cycles(data_csv_jun23_float)\n",
    "detection_aug23 = boundaries_cycles(data_csv_aug23_float)\n",
    "detection_okt23 = boundaries_cycles(data_csv_okt23_float)\n",
    "detection_dec23 = boundaries_cycles(data_csv_dec23_float)\n",
    "\n",
    "#merge test data\n",
    "dfs_data = [data_csv_jun23_float,data_csv_aug23_float,data_csv_okt23_float,data_csv_dec23_float]\n",
    "dfs_detection_cycle = [detection_jun23,detection_aug23,detection_okt23,detection_dec23]\n",
    "\n",
    "file_name= [\"jun23_cycle.txt\" ,\"aug23_cycle.txt\", \n",
    "            \"okt23_cycle.txt\",\"dec23_cycle.txt\"]\n",
    "\n",
    "#train\n",
    "clf_duty_uint8 = train_cycle_classifier(classifier_cycle,X_res, y_res,seed)\n",
    "\n",
    "#compute the sequence for each month of data\n",
    "all_sequences = []\n",
    "for i in range(len(dfs_data)):\n",
    "    data_sequences = find_non_consecutive_sequences(dfs_data[i], dfs_detection_cycle[i],delta,column_name=\"recognized_label\")\n",
    "    all_sequences.append(data_sequences)\n",
    "\n",
    "for i in range(len(dfs_data)):\n",
    "    test_sequences = all_sequences[i]\n",
    "\n",
    "    # Encode the sequences for testing\n",
    "    test_features = [le_state.transform(seq['non_consecutive_labels']) for seq in test_sequences]\n",
    "    test_features_padded = pad_sequences(test_features, max_sequence_length)\n",
    "\n",
    "    #test\n",
    "    y_pred = clf_duty_uint8.predict(test_features_padded)\n",
    "\n",
    "    # save the results in a file\n",
    "    df_temp= pd.DataFrame()\n",
    "    df_temp[\"start\"]=dfs_detection_cycle[i][\"start\"]\n",
    "    df_temp[\"end\"]=dfs_detection_cycle[i][\"end\"]\n",
    "    df_temp[\"label\"]=le_cycle.inverse_transform(y_pred)\n",
    "\n",
    "    if flag_save_results:\n",
    "        folder_path = dir_model_pred + \"int8/\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        create_reference_label_file(folder_path+file_name[i],df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert model to C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i  ./results/c_code/approach3/mlp_int8.tflite > ./results/c_code/approach3/mlp_int8.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlearn import convert\n",
    "\n",
    "cmodel = convert(clf_duty_uint8, method='inline',dtype='uint8_t')\n",
    "cmodel.save(file = model_path + \"clf_duty_uint8.h\", name='clf_duty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['quantization','state_classifier', \"detection mean\",\n",
    "                                    'Abnormal mean F1-score','Normal mean F1-score','Overall mean F1-score'])\n",
    "\n",
    "dir = './results/reference_cycle_labels/'\n",
    "reference_path = dir\n",
    "collar = 202.75\n",
    "\n",
    "classifier_states=[\"mlp\"]\n",
    "classifier_cycles=[\"xtree\"]\n",
    "quantizations=[\"float\",\"int8\"]\n",
    "for classifier_state,classifier_cycle in zip(classifier_states,classifier_cycles):\n",
    "    for q in quantizations:\n",
    "        result_path = dir_model_pred +q+\"/\"\n",
    "        f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal , f1score_normal, precision_normal, recall_normal= compute_classification_sedeval(reference_path,result_path,collar)\n",
    "        detection_file_overall = compute_detection_sedeval(reference_path,result_path,collar)\n",
    "        print(\"---------- \"+classifier_state+ \" - \"+q+\" ----------\")\n",
    "        print(\"DETECTION: \"+ str(detection_file_overall*100) )\n",
    "        print(\"ABNORMAL:\")\n",
    "        print(\"F1-score: \"+ str(f1score_abnormal*100) )\n",
    "        print(\"Precision: \"+ str(precision_abnormal*100) )\n",
    "        print(\"Recall: \"+ str(recall_abnormal*100) )\n",
    "        print(\"NORMAL:\")\n",
    "        print(\"F1-score: \"+ str(f1score_normal*100) )\n",
    "        print(\"Precision: \"+ str(precision_normal*100) )\n",
    "        print(\"Recall: \"+ str(recall_normal*100) )\n",
    "        print(\"OVERALL:\")\n",
    "        print(\"F1-score: \"+ str(f1score_file*100)  )\n",
    "        print(\"Precision: \"+ str(precision_file*100) )\n",
    "        print(\"Recall: \"+ str(recall_file*100) )\n",
    "\n",
    "        dflocal = pd.DataFrame({'quantization':q,\n",
    "                                'state_classifier':classifier_state, \n",
    "                                \"detection mean\": np.mean(detection_file_overall)*100,\n",
    "                                'Abnormal mean F1-score':np.mean(f1score_abnormal)*100,\n",
    "                                'Normal mean F1-score':np.mean(f1score_normal)*100,\n",
    "                                'Overall mean F1-score':np.mean(f1score_file)*100},index=[0])\n",
    "            \n",
    "        df_results = pd.concat([df_results, dflocal], ignore_index=True)\n",
    "\n",
    "df_results.to_csv(dir_model_pred + 'results_mcu.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
