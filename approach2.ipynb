{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook corresponding to the \"Approach-2\" presented in the paper.\n",
    "\n",
    "This is the same approach used in the [\"Tinyml anomaly detection for industrial machines with periodic duty cycles\" (Sensor Application Symposium 2024)](https://ieeexplore.ieee.org/abstract/document/10636584/), and serves as the baseline experiment.\n",
    "\n",
    "Two experiments are carried on:\n",
    "1) As in the SAS2024, the performance is evaluated in leave-one-month-out CV in the original 4 months (called DS1).\n",
    "2) The generalization is evaluated using the whole DS1 for training and the whole DS2 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_functions import *\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data, extract feature and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different functions used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify each previously detected duty-cycle\n",
    "def apply_heuristic_rules(df_testset):\n",
    "    #establish a copy of detected labels, and then assign the normal/abnormal label\n",
    "    df_testset[\"recognized_cycles\"] = df_testset[\"detected_cycles\"].copy(deep=True)\n",
    "\n",
    "    # Create a list to store the non-repeated labels for each cycle\n",
    "    cycle_transition_sequences = []\n",
    "\n",
    "    # Initialize variables to track the start index, previous cycle value, and unique labels within each cycle\n",
    "    start_index = None\n",
    "    prev_cycle = None\n",
    "    transition_sequence = []\n",
    "\n",
    "    # list of valid sequence of Normal cycles\n",
    "    normal_snequences = [\"CDC\", \"CD\"]\n",
    "\n",
    "    # Iterate through the dataframe\n",
    "    for index, row in df_testset.iterrows():\n",
    "        current_cycle = row['detected_cycles']\n",
    "        current_label = row['recognized_label']\n",
    "        \n",
    "        # Check if there is a transition from \"no_cycles\" to \"cycles\"\n",
    "        if prev_cycle == \"No_cycle\" and current_cycle == \"Cycle\":\n",
    "            start_index = index\n",
    "            transition_sequence = []\n",
    "        \n",
    "        # Check if there is a transition from \"cycles\" to \"no_cycles\"\n",
    "        elif prev_cycle == \"Cycle\" and current_cycle == \"No_cycle\":\n",
    "            # Record the transition labels for the current cycle\n",
    "            cycle_transition_sequences.append((start_index, index - 1, ''.join(transition_sequence)))\n",
    "            start_index = None\n",
    "            transition_sequence = []\n",
    "        \n",
    "        # Update the transition sequence within the current cycle\n",
    "        if current_cycle == \"Cycle\":\n",
    "            if transition_sequence and transition_sequence[-1] != current_label:\n",
    "                transition_sequence.append(current_label)\n",
    "            elif not transition_sequence:\n",
    "                transition_sequence.append(current_label)\n",
    "        \n",
    "        # Update the previous cycle value\n",
    "        prev_cycle = current_cycle\n",
    "\n",
    "    # Check if the last cycle is ongoing and record its transition labels if it is\n",
    "    if start_index is not None:\n",
    "        cycle_transition_sequences.append((start_index, len(df_testset) - 1, ''.join(transition_sequence)))\n",
    "\n",
    "    # I need to extend the analysis beyond the limits of the detected duty-cycles to see I there are missing values or \n",
    "    # the machine was off (state A) before/after the cicle\n",
    "    extra_lenght = 3\n",
    "\n",
    "    # Classify the Normal/Abnormal cycle according to the sequence label\n",
    "    for cycle_start, cycle_end, transitions in cycle_transition_sequences:\n",
    "\n",
    "        #first I check if the time difference is equal to 2 minute, otherwise means that there are missing values before/after the duty-cyle \n",
    "        i1 = cycle_start-extra_lenght if cycle_start-extra_lenght > 0 else 0\n",
    "        i2 = cycle_end+extra_lenght if cycle_end+extra_lenght < df_testset.index[-1] else df_testset.index[-1]\n",
    "\n",
    "        temp1 = (df_testset.iloc[cycle_start][\"index\"]-df_testset.iloc[i1][\"index\"]).total_seconds()\n",
    "        temp2 = (df_testset.iloc[i2][\"index\"]-df_testset.iloc[cycle_end][\"index\"]).total_seconds()\n",
    "\n",
    "        bool_condition1 = temp1==(60.0*extra_lenght) and temp2==(60.0*extra_lenght) #no missing values\n",
    "        if extra_lenght==1:\n",
    "            bool_condition2 = df_testset.loc[i1, 'recognized_label']==\"B\" #before was in idle state\n",
    "            bool_condition3 = df_testset.loc[i2, 'recognized_label']==\"B\"#after was in idle state\n",
    "        elif extra_lenght==2:\n",
    "            bool_condition2 = df_testset.loc[i1, 'recognized_label']==\"B\" or df_testset.loc[i1+1, 'recognized_label']==\"B\" or df_testset.loc[i1+2, 'recognized_label']==\"B\" #before was in idle state\n",
    "            bool_condition3 = df_testset.loc[i2, 'recognized_label']==\"B\" or df_testset.loc[i2-1, 'recognized_label']==\"B\" or df_testset.loc[i2-2, 'recognized_label']==\"B\" #after was in idle state\n",
    "        elif extra_lenght==3:\n",
    "            bool_condition2 = df_testset.loc[i1, 'recognized_label']==\"B\" or df_testset.loc[i1+1, 'recognized_label']==\"B\" #before was in idle state\n",
    "            bool_condition3 = df_testset.loc[i2, 'recognized_label']==\"B\" or df_testset.loc[i2-1, 'recognized_label']==\"B\" #after was in idle state\n",
    "\n",
    "        if bool_condition1 and bool_condition2 and bool_condition3 and (transitions in normal_snequences): #check the sequence inside the cycle\n",
    "            df_testset.loc[cycle_start:cycle_end, 'recognized_cycles'] = \"Normal\"\n",
    "\n",
    "        else:\n",
    "            df_testset.loc[cycle_start:cycle_end, 'recognized_cycles'] = \"Abnormal\"\n",
    "\n",
    "    return\n",
    "\n",
    "#extract the duty-cycle bouts in a date range\n",
    "def create_segments_cycle_classified(start_date,end_date,df_testset):\n",
    "    complete_range = pd.date_range(start=start_date, end=end_date, freq='1min')\n",
    "    complete_df = pd.DataFrame({'index': complete_range})\n",
    "\n",
    "    filtered_df = df_testset[(df_testset['index'] >= start_date) & (df_testset['index'] <= end_date)]\n",
    "\n",
    "    merged_df = pd.merge(complete_df, filtered_df, on='index', how='left')\n",
    "\n",
    "    #Compute the start and end of each cycle\n",
    "    merged_df['group'] = (merged_df['recognized_cycles'] != merged_df['recognized_cycles'].shift()).cumsum()\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for group_name, group_data in merged_df.groupby('group'):\n",
    "        label = group_data[\"recognized_cycles\"].iloc[0]\n",
    "        if (label != None):\n",
    "            start = group_data[\"index\"].iloc[0]\n",
    "            finish = group_data[\"index\"].iloc[-1]\n",
    "            data_to_append = {'start': start, 'end': finish, 'label': label}\n",
    "            df = pd.DataFrame(data_to_append,index=[group_name])\n",
    "            dfs.append(df)\n",
    "\n",
    "    df_classified_cycle = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    df_classified_cycle = df_classified_cycle[df_classified_cycle['label'] != 'No_cycle']\n",
    "    df_classified_cycle.dropna(subset=['label'], inplace=True)\n",
    "    df_classified_cycle = df_classified_cycle.reset_index(drop=True)\n",
    "\n",
    "    return df_classified_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"../../../data/\"\n",
    "#first 4 months of data (DS1)\n",
    "data_csv_jun21 = read_month_data(directory+'Confidential_Drive_data_Jun2021.csv',1)\n",
    "data_csv_okt21 = read_month_data(directory+'Confidential_Drive_data_Okt2021.csv',1)\n",
    "data_csv_jan22 = read_month_data(directory+'Confidential_Drive_data_Jan2022.csv',1)\n",
    "data_csv_april22 = read_month_data(directory+'Confidential_Drive_data_April2022.csv',1)\n",
    "#new 4 months (DS2)\n",
    "data_csv_jun23 = read_month_data(directory+'Confidential_Drive_data_June2023_Drift20.csv')\n",
    "data_csv_aug23 = read_month_data(directory+'Confidential_Drive_data_Aug2023_Drift20.csv')\n",
    "data_csv_okt23 = read_month_data(directory+'Confidential_Drive_data_Oct2023_Drift20.csv')\n",
    "data_csv_dec23 = read_month_data(directory+'Confidential_Drive_data_Dec2023_Drift20.csv')\n",
    "\n",
    "#re-order the column name to be consistent with the previous csv files\n",
    "desired_order=[\"High-pressure\",\"Low-pressure\",\"Speed\"]\n",
    "data_csv_jun23=data_csv_jun23[desired_order]\n",
    "data_csv_aug23=data_csv_aug23[desired_order]\n",
    "data_csv_okt23=data_csv_okt23[desired_order]\n",
    "data_csv_dec23=data_csv_dec23[desired_order]\n",
    "\n",
    "#These data has duplicated entries\n",
    "data_csv_okt23 = data_csv_okt23[~data_csv_okt23.index.duplicated(keep='first')]\n",
    "\n",
    "# round to zero speed less than zero\n",
    "data_csv_jun21.loc[data_csv_jun21['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_okt21.loc[data_csv_okt21['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_jan22.loc[data_csv_jan22['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_april22.loc[data_csv_april22['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_jun23.loc[data_csv_jun23['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_aug23.loc[data_csv_aug23['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_okt23.loc[data_csv_okt23['Speed'] < 0 , 'Speed'] = 0\n",
    "data_csv_dec23.loc[data_csv_dec23['Speed'] < 0 , 'Speed'] = 0\n",
    "\n",
    "\n",
    "# complete the dataset with missing values\n",
    "full_timestamp = pd.date_range(start = data_csv_jun21.index[0], end = data_csv_jun21.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_jun21 = data_csv_jun21.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_okt21.index[0], end = data_csv_okt21.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_okt21 = data_csv_okt21.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_jan22.index[0], end = data_csv_jan22.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_jan22 = data_csv_jan22.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_april22.index[0], end = data_csv_april22.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_april22 = data_csv_april22.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_jun23.index[0], end = data_csv_jun23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_jun23 = data_csv_jun23.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_aug23.index[0], end = data_csv_aug23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_aug23 = data_csv_aug23.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_okt23.index[0], end = data_csv_okt23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_okt23 = data_csv_okt23.reindex(full_timestamp)\n",
    "\n",
    "full_timestamp = pd.date_range(start = data_csv_dec23.index[0], end = data_csv_dec23.index[-1],inclusive=\"both\",freq=\"1min\" )\n",
    "data_csv_dec23 = data_csv_dec23.reindex(full_timestamp)\n",
    "\n",
    "\n",
    "\n",
    "#use linear interpolation for the NaN missing values\n",
    "interpolate_values(data_csv_jun21)\n",
    "interpolate_values(data_csv_okt21)\n",
    "interpolate_values(data_csv_jan22)\n",
    "interpolate_values(data_csv_april22)\n",
    "interpolate_values(data_csv_jun23)\n",
    "interpolate_values(data_csv_aug23)\n",
    "interpolate_values(data_csv_okt23)\n",
    "interpolate_values(data_csv_dec23)\n",
    "\n",
    "del desired_order, directory, full_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_csv = [data_csv_jun21,data_csv_okt21,data_csv_jan22,data_csv_april22,data_csv_jun23,data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "for data in list_data_csv:\n",
    "    extract_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ground truth reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files from imagimob\n",
    "directory=\"../../data/\"\n",
    "column_interest=['Time(Seconds)' , 'Length(Seconds)',\"Label(string)\"]\n",
    "\n",
    "#read labels of states\n",
    "file_imagimob_1 = pd.read_csv(directory+\"April_2022/Label.label\",usecols=column_interest)\n",
    "file_imagimob_2 = pd.read_csv(directory+\"Jan_2022/Label.label\",usecols=column_interest)\n",
    "file_imagimob_3 = pd.read_csv(directory+\"Jun_2021/Label.label\",usecols=column_interest)\n",
    "file_imagimob_4 = pd.read_csv(directory+\"Okt_2021/Label.label\",usecols=column_interest)\n",
    "\n",
    "timestamps_april2022 = df_timestamps(file_imagimob_1)\n",
    "timestamps_jan2022 = df_timestamps(file_imagimob_2)\n",
    "timestamps_jun2021 = df_timestamps(file_imagimob_3)\n",
    "timestamps_okt2021 = df_timestamps(file_imagimob_4)\n",
    "\n",
    "#read labels of duty-cycle\n",
    "file_imagimob_1 = pd.read_csv(directory+\"April_2022/Label_cycle.label\",usecols=column_interest)\n",
    "file_imagimob_2 = pd.read_csv(directory+\"Jan_2022/Label_cycle.label\",usecols=column_interest)\n",
    "file_imagimob_3 = pd.read_csv(directory+\"Jun_2021/Label_cycle.label\",usecols=column_interest)\n",
    "file_imagimob_4 = pd.read_csv(directory+\"Okt_2021/Label_cycle.label\",usecols=column_interest)\n",
    "\n",
    "timestamps_cycle_april2022 = df_timestamps(file_imagimob_1)\n",
    "timestamps_cycle_jan2022 = df_timestamps(file_imagimob_2)\n",
    "timestamps_cycle_jun2021 = df_timestamps(file_imagimob_3)\n",
    "timestamps_cycle_okt2021 = df_timestamps(file_imagimob_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate vector with the labels of reference (states)\n",
    "downsampled_freq='1T'\n",
    "true_label_april22 = ndarray_labels(datetime.datetime(2022, 4, 1),datetime.datetime(2022, 5, 1),timestamps_april2022,downsampled_freq)\n",
    "true_label_jan22 = ndarray_labels(datetime.datetime(2021, 12, 21),datetime.datetime(2022, 1, 21),timestamps_jan2022,downsampled_freq)\n",
    "true_label_jun21 = ndarray_labels(datetime.datetime(2021, 6, 1),datetime.datetime(2021, 7, 1),timestamps_jun2021,downsampled_freq)\n",
    "true_label_okt21 = ndarray_labels(datetime.datetime(2021, 10, 1),datetime.datetime(2021, 11, 1),timestamps_okt2021,downsampled_freq)\n",
    "\n",
    "#generate vector with the labels of reference (duty-cycle)\n",
    "true_label_cycle_april22 = ndarray_labels(datetime.datetime(2022, 4, 1),datetime.datetime(2022, 5, 1),timestamps_cycle_april2022,downsampled_freq)\n",
    "true_label_cycle_jan22 = ndarray_labels(datetime.datetime(2021, 12, 21),datetime.datetime(2022, 1, 21),timestamps_cycle_jan2022,downsampled_freq)\n",
    "true_label_cycle_jun21 = ndarray_labels(datetime.datetime(2021, 6, 1),datetime.datetime(2021, 7, 1),timestamps_cycle_jun2021,downsampled_freq)\n",
    "true_label_cycle_okt21 = ndarray_labels(datetime.datetime(2021, 10, 1),datetime.datetime(2021, 11, 1),timestamps_cycle_okt2021,downsampled_freq)\n",
    "\n",
    "true_label_cycle_april22 = np.where(true_label_cycle_april22 == None, 'No_cycle', true_label_cycle_april22)\n",
    "true_label_cycle_jan22 = np.where(true_label_cycle_jan22 == None, 'No_cycle', true_label_cycle_jan22)\n",
    "true_label_cycle_jun21 = np.where(true_label_cycle_jun21 == None, 'No_cycle', true_label_cycle_jun21)\n",
    "true_label_cycle_okt21 = np.where(true_label_cycle_okt21 == None, 'No_cycle', true_label_cycle_okt21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imput ground-truth duty-cycle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files from imagimob\n",
    "directory=\"../../data/\"\n",
    "#read labels of duty-cycle\n",
    "labels_jun21 = import_cycle_labels(directory+\"Jun_2021/Label_cycle.label\")\n",
    "labels_okt21 = import_cycle_labels(directory+\"Okt_2021/Label_cycle.label\")\n",
    "labels_jan22 = import_cycle_labels(directory+\"Jan_2022/Label_cycle.label\")\n",
    "labels_april22 = import_cycle_labels(directory+\"April_2022/Label_cycle.label\")\n",
    "labels_jun23 = import_cycle_labels(directory+\"June_23/Label_cycle.label\")\n",
    "labels_aug23 = import_cycle_labels(directory+\"Aug_23/Label_cycle.label\")\n",
    "labels_okt23 = import_cycle_labels(directory+\"Okt_23/Label_cycle.label\")\n",
    "labels_dec23 = import_cycle_labels(directory+\"Dec_23/Label_cycle.label\")\n",
    "\n",
    "\n",
    "for data in [labels_jun23,labels_aug23,labels_okt23,labels_dec23]:\n",
    "    replace_labels_cycles(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_jun21 [true_label_jun21=='E']='B'\n",
    "true_label_okt21 [true_label_okt21=='E']='B'\n",
    "true_label_jan22 [true_label_jan22=='E']='B'\n",
    "true_label_april22 [true_label_april22=='E']='B'\n",
    "\n",
    "data_DS1=[data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22]\n",
    "data_DS2=[data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23]\n",
    "\n",
    "true_state_labels_DS1=[true_label_jun21, true_label_okt21,true_label_jan22,true_label_april22]\n",
    "df_testset_DS2= pd.concat(data_DS2)\n",
    "\n",
    "file_name_states_DS1= [\"jun2021_state.txt\" ,\"okt2021_state.txt\",\"jan2022_state.txt\",\"april2022_state.txt\"]\n",
    "file_name_cycles_DS1= [\"jun2021_cycle.txt\" ,\"okt2021_cycle.txt\",\"jan2022_cycle.txt\",\"april2022_cycle.txt\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "dir_exp1 = \"./results/approach2/DS1/\"\n",
    "dir_exp2 = \"./results/approach2/DS2/\"\n",
    "\n",
    "flag_save_results=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete not requires variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del timestamps_april2022, timestamps_jan2022, timestamps_jun2021, timestamps_okt2021\n",
    "del file_imagimob_1,file_imagimob_2,file_imagimob_3,file_imagimob_4, column_interest, directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Train/test on DS1 using leave-one-month CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "for i in range(len(data_DS1)):\n",
    "    # train_set\n",
    "    df_dataset = pd.concat([data for j, data in enumerate(data_DS1) if j != i])\n",
    "    df_dataset[\"ref_label\"]= np.concatenate([data for j, data in enumerate(true_state_labels_DS1) if j != i])\n",
    "\n",
    "    removed_indices = df_dataset[df_dataset['ref_label'].isnull()].index.tolist()\n",
    "    df_dataset = df_dataset[df_dataset['ref_label'].notnull()]\n",
    "    df_dataset=df_dataset.reset_index()\n",
    "\n",
    "    x_train = df_dataset[df_dataset.columns[1:-1]]\n",
    "    y_train = df_dataset[df_dataset.columns[-1]]\n",
    "    x_train_balanced, y_train_balanced, le = balance_dataset(x_train,y_train)\n",
    "    x_train_balanced=pd.DataFrame(scaler.fit_transform(x_train_balanced), columns=x_train.columns)\n",
    "    \n",
    "    #test_set\n",
    "    x_test=pd.DataFrame(scaler.transform(data_DS1[i]), columns=x_train.columns)\n",
    "\n",
    "    for seed in seeds:\n",
    "        for classifier in classifiers:\n",
    "            #train/test states\n",
    "            clf = train_state_supervised_classifier(classifier,x_train_balanced, y_train_balanced,seed)\n",
    "            y_predict = clf.predict(x_test)\n",
    "\n",
    "            #apply 3rd median filter\n",
    "            y_pred_smoothed = smooth_labels(y_predict,3)\n",
    "            \n",
    "            y_recognized=le.inverse_transform(y_pred_smoothed.astype(int))\n",
    "            df_temp=data_DS1[i].copy()\n",
    "            df_temp[\"recognized_label\"]=y_recognized\n",
    "\n",
    "            # df_temp=df_testset.copy()\n",
    "            df_temp=df_temp.reset_index()\n",
    "\n",
    "            #classify duty-cycle\n",
    "            df_recognized_states = create_segments_state(data_DS1[i].index[0],data_DS1[i].index[-1],df_temp)\n",
    "\n",
    "            # save the results in files\n",
    "            if flag_save_results:\n",
    "                folder_path = dir_exp1+classifier+\"/\"+str(seed)+\"/\"\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                create_reference_label_file(folder_path+file_name_states_DS1[i],df_recognized_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "classifiers=[\"nb\",\"xgboost\"]\n",
    "\n",
    "for i in range(len(data_DS1)):\n",
    "    # train_set\n",
    "    df_dataset = pd.concat([data for j, data in enumerate(data_DS1) if j != i])\n",
    "    df_dataset[\"ref_label\"]= np.concatenate([data for j, data in enumerate(true_state_labels_DS1) if j != i])\n",
    "\n",
    "    removed_indices = df_dataset[df_dataset['ref_label'].isnull()].index.tolist()\n",
    "    df_dataset = df_dataset[df_dataset['ref_label'].notnull()]\n",
    "    df_dataset=df_dataset.reset_index()\n",
    "\n",
    "    x_train = df_dataset[df_dataset.columns[1:-1]]\n",
    "    y_train = df_dataset[df_dataset.columns[-1]]\n",
    "    x_train_balanced, y_train_balanced, le = balance_dataset(x_train,y_train)\n",
    "    x_train_balanced=pd.DataFrame(scaler.fit_transform(x_train_balanced), columns=x_train.columns)\n",
    "    \n",
    "    #test_set\n",
    "    x_test=pd.DataFrame(scaler.transform(data_DS1[i]), columns=x_train.columns)\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        #train/test states\n",
    "        clf = train_state_supervised_classifier(classifier,x_train_balanced, y_train_balanced,seed)\n",
    "        y_predict = clf.predict(x_test)\n",
    "\n",
    "        #apply 3rd median filter\n",
    "        y_pred_smoothed = smooth_labels(y_predict,3)\n",
    "        \n",
    "        y_recognized=le.inverse_transform(y_pred_smoothed.astype(int))\n",
    "        df_temp=data_DS1[i].copy()\n",
    "        df_temp[\"recognized_label\"]=y_recognized\n",
    "\n",
    "        # df_temp=df_testset.copy()\n",
    "        df_temp=df_temp.reset_index()\n",
    "\n",
    "        #classify duty-cycle\n",
    "        df_recognized_states = create_segments_state(data_DS1[i].index[0],data_DS1[i].index[-1],df_temp)\n",
    "\n",
    "        # save the results in files\n",
    "        if flag_save_results:\n",
    "            folder_path = dir_exp1+classifier+\"/\"\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            create_reference_label_file(folder_path+file_name_states_DS1[i],df_recognized_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have all the predicted state labels, I can apply detection using a threshold and its subsequent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_freq='1T'\n",
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "# classifier with seeds parameters\n",
    "for seed in seeds:\n",
    "    for classifier in classifiers:\n",
    "    \n",
    "        #import classified state_labels\n",
    "        folder_path = dir_exp1+classifier+\"/\"+str(seed)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/jun2021_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_jun21[\"recognized_label\"] = ndarray_labels(data_csv_jun21.index[0],data_csv_jun21.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/okt2021_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_okt21[\"recognized_label\"] = ndarray_labels(data_csv_okt21.index[0],data_csv_okt21.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/jan2022_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_jan22[\"recognized_label\"] = ndarray_labels(data_csv_jan22.index[0],data_csv_jan22.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/april2022_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_april22[\"recognized_label\"] = ndarray_labels(data_csv_april22.index[0],data_csv_april22.index[-1],aux,downsampled_freq)\n",
    "        #form testset\n",
    "        df_testset= pd.concat([data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22])\n",
    "\n",
    "        #apply detection threshold\n",
    "        df_testset.reset_index(inplace=True)\n",
    "        df_testset[\"detected_cycles\"]='No_cycle'\n",
    "        df_testset.loc[df_testset['Speed_order3'] > 2.5, 'detected_cycles'] = 'Cycle'\n",
    "\n",
    "        #classify duty_cycle\n",
    "        apply_heuristic_rules(df_testset)\n",
    "\n",
    "        #save results\n",
    "        df_recognized_cycles_jun21 = create_segments_cycle_classified(data_csv_jun21.index[0],data_csv_jun21.index[-1],df_testset=df_testset)\n",
    "        df_recognized_cycles_okt21 = create_segments_cycle_classified(data_csv_okt21.index[0],data_csv_okt21.index[-1],df_testset=df_testset)\n",
    "        df_recognized_cycles_jan22 = create_segments_cycle_classified(data_csv_jan22.index[0],data_csv_jan22.index[-1],df_testset=df_testset)\n",
    "        df_recognized_cycles_april22 = create_segments_cycle_classified(data_csv_april22.index[0],data_csv_april22.index[-1],df_testset=df_testset)\n",
    "        if flag_save_results:\n",
    "            create_reference_label_file(folder_path+\"/jun2021_cycle.txt\",df_recognized_cycles_jun21)\n",
    "            create_reference_label_file(folder_path+\"/okt2021_cycle.txt\",df_recognized_cycles_okt21)\n",
    "            create_reference_label_file(folder_path+\"/jan2022_cycle.txt\",df_recognized_cycles_jan22)\n",
    "            create_reference_label_file(folder_path+\"/april2022_cycle.txt\",df_recognized_cycles_april22)\n",
    "            print(\"Results saved in: \"+folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# classifier without seeds parameters\n",
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "\n",
    "for classifier in classifiers:\n",
    "\n",
    "    #import classified state_labels\n",
    "    folder_path = dir_exp1+classifier\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/jun2021_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_jun21[\"recognized_label\"] = ndarray_labels(data_csv_jun21.index[0],data_csv_jun21.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/okt2021_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_okt21[\"recognized_label\"] = ndarray_labels(data_csv_okt21.index[0],data_csv_okt21.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/jan2022_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_jan22[\"recognized_label\"] = ndarray_labels(data_csv_jan22.index[0],data_csv_jan22.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/april2022_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_april22[\"recognized_label\"] = ndarray_labels(data_csv_april22.index[0],data_csv_april22.index[-1],aux,downsampled_freq)\n",
    "\n",
    "    #form testset\n",
    "    df_testset= pd.concat([data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22])\n",
    "\n",
    "    #apply detection threshold\n",
    "    df_testset.reset_index(inplace=True)\n",
    "    df_testset[\"detected_cycles\"]='No_cycle'\n",
    "    df_testset.loc[df_testset['Speed_order3'] > 2.5, 'detected_cycles'] = 'Cycle'\n",
    "\n",
    "    #classify duty_cycle\n",
    "    apply_heuristic_rules(df_testset)\n",
    "\n",
    "    #save results\n",
    "    df_recognized_cycles_jun21 = create_segments_cycle_classified(data_csv_jun21.index[0],data_csv_jun21.index[-1],df_testset=df_testset)\n",
    "    df_recognized_cycles_okt21 = create_segments_cycle_classified(data_csv_okt21.index[0],data_csv_okt21.index[-1],df_testset=df_testset)\n",
    "    df_recognized_cycles_jan22 = create_segments_cycle_classified(data_csv_jan22.index[0],data_csv_jan22.index[-1],df_testset=df_testset)\n",
    "    df_recognized_cycles_april22 = create_segments_cycle_classified(data_csv_april22.index[0],data_csv_april22.index[-1],df_testset=df_testset)\n",
    "\n",
    "    if flag_save_results:\n",
    "        create_reference_label_file(folder_path+\"/jun2021_cycle.txt\",df_recognized_cycles_jun21)\n",
    "        create_reference_label_file(folder_path+\"/okt2021_cycle.txt\",df_recognized_cycles_okt21)\n",
    "        create_reference_label_file(folder_path+\"/jan2022_cycle.txt\",df_recognized_cycles_jan22)\n",
    "        create_reference_label_file(folder_path+\"/april2022_cycle.txt\",df_recognized_cycles_april22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_exp1 = pd.DataFrame(columns=['state_classifier', \"detection mean\",'detection std',\n",
    "                                        'Abnormal mean F1-score','Abnormal std F1-score',\n",
    "                                        'Normal mean F1-score','Normal std F1-score',\n",
    "                                        'Overall mean F1-score','Overall std F1-score'])\n",
    "\n",
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "dir = './results/reference_cycle_labels/'\n",
    "reference_path = dir\n",
    "collar = 202.75\n",
    "\n",
    "detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "detection_files_overall, f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[],[]\n",
    "\n",
    "for classifier_state in classifiers:\n",
    "    detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "    detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "    detection_files_overall, f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[],[]\n",
    "\n",
    "    for seed_cycle in seeds:\n",
    "        result_path = dir_exp1+classifier_state+\"/\"+str(seed_cycle)+\"/\"\n",
    "        f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal , f1score_normal, precision_normal, recall_normal= compute_classification_sedeval(reference_path,result_path,collar)\n",
    "        detection_file_overall = compute_detection_sedeval(reference_path,result_path,collar)\n",
    "        detection_files_overall.append(detection_file_overall)\n",
    "        f1score_files_overall.append(f1score_file)\n",
    "        precision_files_overall.append(precision_file)\n",
    "        recall_files_overall.append(recall_file)\n",
    "        f1score_files_abnormal.append(f1score_abnormal)\n",
    "        precision_files_abnormal.append(precision_abnormal)\n",
    "        recall_files_abnormal.append(recall_abnormal)\n",
    "        f1score_files_normal.append(f1score_normal)\n",
    "        precision_files_normal.append(precision_normal)\n",
    "        recall_files_normal.append(recall_normal)\n",
    "\n",
    "    print(\"---------- \"+classifier_state+\" ----------\")\n",
    "    print(\"DETECTION: \"+ str(np.mean(detection_files_overall)*100) +\" - \"+ str(np.std(detection_files_overall)*100) )\n",
    "    print(\"ABNORMAL:\")\n",
    "    print(\"F1-score: \"+ str(np.mean(f1score_files_abnormal)*100) +\" - \"+ str(np.std(f1score_files_abnormal)*100) )\n",
    "    print(\"Precision: \"+ str(np.mean(precision_files_abnormal)*100) +\" - \"+ str(np.std(precision_files_abnormal)*100) )\n",
    "    print(\"Recall: \"+ str(np.mean(recall_files_abnormal)*100) +\" - \"+ str(np.std(recall_files_abnormal)*100) )\n",
    "    print(\"NORMAL:\")\n",
    "    print(\"F1-score: \"+ str(np.mean(f1score_files_normal)*100) +\" - \"+ str(np.std(f1score_files_normal)*100) )\n",
    "    print(\"Precision: \"+ str(np.mean(precision_files_normal)*100) +\" - \"+ str(np.std(precision_files_normal)*100) )\n",
    "    print(\"Recall: \"+ str(np.mean(recall_files_normal)*100) +\" - \"+ str(np.std(recall_files_normal)*100) )\n",
    "    print(\"OVERALL:\")\n",
    "    print(\"F1-score: \"+ str(np.mean(f1score_files_overall)*100) +\" - \"+ str(np.std(f1score_files_overall)*100) )\n",
    "    print(\"Precision: \"+ str(np.mean(precision_files_overall)*100) +\" - \"+ str(np.std(precision_files_overall)*100) )\n",
    "    print(\"Recall: \"+ str(np.mean(recall_files_overall)*100) +\" - \"+ str(np.std(recall_files_overall)*100) )\n",
    "\n",
    "    dflocal = pd.DataFrame({'state_classifier':classifier_state, \n",
    "                            \"detection mean\": np.mean(detection_files_overall)*100,\n",
    "                            'detection std': np.std(detection_files_overall)*100,\n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100},index=[0])\n",
    "        \n",
    "    df_results_exp1 = pd.concat([df_results_exp1, dflocal], ignore_index=True)\n",
    "\n",
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "\n",
    "detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "detection_files_overall, f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[],[]\n",
    "\n",
    "for classifier_state in classifiers:\n",
    "    result_path = dir_exp1+classifier_state+\"/\"\n",
    "    f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal , f1score_normal, precision_normal, recall_normal= compute_classification_sedeval(reference_path,result_path,collar)\n",
    "    detection_file_overall = compute_detection_sedeval(reference_path,result_path,collar)\n",
    "\n",
    "    print(\"---------- \"+classifier_state+\" ----------\")\n",
    "    print(\"DETECTION: \"+ str(detection_file_overall*100) )\n",
    "    print(\"ABNORMAL:\")\n",
    "    print(\"F1-score: \"+ str(f1score_abnormal*100) )\n",
    "    print(\"Precision: \"+ str(precision_abnormal*100) )\n",
    "    print(\"Recall: \"+ str(recall_abnormal*100) )\n",
    "    print(\"NORMAL:\")\n",
    "    print(\"F1-score: \"+ str(f1score_normal*100) )\n",
    "    print(\"Precision: \"+ str(precision_normal*100) )\n",
    "    print(\"Recall: \"+ str(recall_normal*100) )\n",
    "    print(\"OVERALL:\")\n",
    "    print(\"F1-score: \"+ str(f1score_file*100)  )\n",
    "    print(\"Precision: \"+ str(precision_file*100) )\n",
    "    print(\"Recall: \"+ str(recall_file*100) )\n",
    "\n",
    "    dflocal = pd.DataFrame({'state_classifier':classifier_state, \n",
    "                            \"detection mean\": np.mean(detection_file_overall)*100,\n",
    "                            'Abnormal mean F1-score':np.mean(f1score_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_file)*100},index=[0])\n",
    "        \n",
    "    df_results_exp1 = pd.concat([df_results_exp1, dflocal], ignore_index=True)\n",
    "\n",
    "df_results_exp1.to_csv(dir_exp1 + 'experiment1_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "Train in DS1 and test in DS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset= pd.concat([data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22])\n",
    "df_dataset[\"ref_label\"]= np.concatenate((true_label_jun21, true_label_okt21,true_label_jan22,true_label_april22), axis=0)    \n",
    "df_dataset[\"ref_label_cycle\"]= np.concatenate((true_label_cycle_jun21, true_label_cycle_okt21,true_label_cycle_jan22,true_label_cycle_april22), axis=0)    \n",
    "\n",
    "removed_indices = df_dataset[df_dataset['ref_label'].isnull()].index.tolist()\n",
    "df_dataset = df_dataset[df_dataset['ref_label'].notnull()]\n",
    "df_dataset=df_dataset.reset_index()\n",
    "\n",
    "# remove the recognized_label column added in the experiment1\n",
    "if 'recognized_label' in df_dataset.columns:\n",
    "    df_dataset = df_dataset.drop('recognized_label', axis=1)\n",
    "\n",
    "x = df_dataset[df_dataset.columns[1:-2]]\n",
    "y_cycle = df_dataset[df_dataset.columns[-1]]\n",
    "y_state = df_dataset[df_dataset.columns[-2]]\n",
    "\n",
    "# normalize feature to range [0;1]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x,4)\n",
    "x = pd.DataFrame(scaler.transform(x), columns=x.columns)\n",
    "\n",
    "y_state [y_state=='E']='B'\n",
    "\n",
    "flag_save_results=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance ds1 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_balanced, y_train_balanced,le = balance_dataset(x,y_state)\n",
    "\n",
    "# Print balanced dataset\n",
    "unique_values, counts = np.unique(y_state, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "print(\"Value class-counts in Unbalanced dataset:\",value_counts)\n",
    "print(\"Value class-porcentage in Unbalanced dataset:\",value_porcentages)\n",
    "\n",
    "unique_values, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "print(\"Value class-counts in Balanced dataset:\",value_counts)\n",
    "print(\"Value class-porcentage in Balanced dataset:\",value_porcentages)\n",
    "\n",
    "del unique_values,counts,value_counts,value_porcentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data (DS2)\n",
    "df_testset= pd.concat([data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23])\n",
    "x_test = pd.DataFrame(scaler.transform(df_testset), columns=df_testset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the state classifiers using only the data belonging to duty-cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "for seed in seeds:\n",
    "    for classifier in classifiers:\n",
    "        #train\n",
    "        clf = train_state_supervised_classifier(classifier,x_train_balanced, y_train_balanced,seed)\n",
    "\n",
    "        #test state-cycles\n",
    "        y_predict = clf.predict(x_test)\n",
    "\n",
    "        #apply 3rd median filter\n",
    "        y_pred_smoothed = smooth_labels(y_predict,3)\n",
    "        \n",
    "        y_recognized=le.inverse_transform(y_pred_smoothed.astype(int))\n",
    "        df_testset[\"recognized_label\"]=y_recognized\n",
    "\n",
    "        df_temp=df_testset.copy()\n",
    "        df_temp=df_temp.reset_index()\n",
    "\n",
    "        \n",
    "        df_recognized_states_jun23 = create_segments_state(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_temp)\n",
    "        df_recognized_states_aug23 = create_segments_state(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_temp)\n",
    "        df_recognized_states_okt23 = create_segments_state(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_temp)\n",
    "        df_recognized_states_dec23 = create_segments_state(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_temp)\n",
    "        \n",
    "        # save the results in files\n",
    "        if flag_save_results:\n",
    "            folder_path = dir_exp2+classifier+\"/\"+str(seed)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            create_reference_label_file(folder_path+\"/jun23_state.txt\",df_recognized_states_jun23)\n",
    "            create_reference_label_file(folder_path+\"/aug23_state.txt\",df_recognized_states_aug23)\n",
    "            create_reference_label_file(folder_path+\"/okt23_state.txt\",df_recognized_states_okt23)\n",
    "            create_reference_label_file(folder_path+\"/dec23_state.txt\",df_recognized_states_dec23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "for classifier in classifiers:\n",
    "    #train\n",
    "    clf = train_state_supervised_classifier(classifier,x_train_balanced, y_train_balanced,seed)\n",
    "\n",
    "    #test state-cycles\n",
    "    y_predict = clf.predict(x_test)\n",
    "\n",
    "    #apply 3rd median filter\n",
    "    y_pred_smoothed = smooth_labels(y_predict,3)\n",
    "    \n",
    "    y_recognized=le.inverse_transform(y_pred_smoothed.astype(int))\n",
    "    df_testset[\"recognized_label\"]=y_recognized\n",
    "\n",
    "    df_temp=df_testset.copy()\n",
    "    df_temp=df_temp.reset_index()\n",
    "\n",
    "    #classify duty-cycle\n",
    "    df_recognized_states_jun23 = create_segments_state(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_temp)\n",
    "    df_recognized_states_aug23 = create_segments_state(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_temp)\n",
    "    df_recognized_states_okt23 = create_segments_state(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_temp)\n",
    "    df_recognized_states_dec23 = create_segments_state(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_temp)\n",
    "\n",
    "    # save the results in files\n",
    "    if flag_save_results:\n",
    "        folder_path = dir_exp2+classifier\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        create_reference_label_file(folder_path+\"/jun23_state.txt\",df_recognized_states_jun23)\n",
    "        create_reference_label_file(folder_path+\"/aug23_state.txt\",df_recognized_states_aug23)\n",
    "        create_reference_label_file(folder_path+\"/okt23_state.txt\",df_recognized_states_okt23)\n",
    "        create_reference_label_file(folder_path+\"/dec23_state.txt\",df_recognized_states_dec23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have all the state labels, I can apply detection using a threshold and its subsequent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_freq='1T'\n",
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "# classifier with seeds parameters\n",
    "for seed in seeds:\n",
    "    for classifier in classifiers:\n",
    "    \n",
    "        #import classified state_labels\n",
    "        folder_path = dir_exp2+classifier+\"/\"+str(seed)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "        aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "        data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "        #form testset\n",
    "        df_testset= pd.concat([data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23])\n",
    "\n",
    "        #apply detection threshold\n",
    "        df_testset.reset_index(inplace=True)\n",
    "        df_testset[\"detected_cycles\"]='No_cycle'\n",
    "        df_testset.loc[df_testset['Speed_order3'] > 2.5, 'detected_cycles'] = 'Cycle'\n",
    "\n",
    "        #classify duty_cycle\n",
    "        apply_heuristic_rules(df_testset)\n",
    "\n",
    "        #save results\n",
    "        df_recognized_cycles_jun23 = create_segments_cycle_classified(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_testset=df_testset)\n",
    "        df_recognized_cycles_aug23 = create_segments_cycle_classified(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_testset=df_testset)\n",
    "        df_recognized_cycles_okt23 = create_segments_cycle_classified(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_testset=df_testset)\n",
    "        df_recognized_cycles_dec23 = create_segments_cycle_classified(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_testset=df_testset)\n",
    "\n",
    "        if flag_save_results:\n",
    "            create_reference_label_file(folder_path+\"/jun23_cycle.txt\",df_recognized_cycles_jun23)\n",
    "            create_reference_label_file(folder_path+\"/aug23_cycle.txt\",df_recognized_cycles_aug23)\n",
    "            create_reference_label_file(folder_path+\"/okt23_cycle.txt\",df_recognized_cycles_okt23)\n",
    "            create_reference_label_file(folder_path+\"/dec23_cycle.txt\",df_recognized_cycles_dec23)\n",
    "\n",
    "# classifier without seeds parameters\n",
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "\n",
    "for classifier in classifiers:\n",
    "\n",
    "    #import classified state_labels\n",
    "    folder_path = \"../results/recognized/experiment_supervised2/DS2/\"+classifier\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "    aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "    data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "    #form testset\n",
    "    df_testset= pd.concat([data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23])\n",
    "\n",
    "    #apply detection threshold\n",
    "    df_testset.reset_index(inplace=True)\n",
    "    df_testset[\"detected_cycles\"]='No_cycle'\n",
    "    df_testset.loc[df_testset['Speed_order3'] > 2.5, 'detected_cycles'] = 'Cycle'\n",
    "\n",
    "    #classify duty_cycle\n",
    "    apply_heuristic_rules(df_testset)\n",
    "\n",
    "    #save results\n",
    "    df_recognized_cycles_jun23 = create_segments_cycle_classified(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_testset=df_testset)\n",
    "    df_recognized_cycles_aug23 = create_segments_cycle_classified(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_testset=df_testset)\n",
    "    df_recognized_cycles_okt23 = create_segments_cycle_classified(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_testset=df_testset)\n",
    "    df_recognized_cycles_dec23 = create_segments_cycle_classified(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_testset=df_testset)\n",
    "\n",
    "    if flag_save_results:\n",
    "        create_reference_label_file(folder_path+\"/jun23_cycle.txt\",df_recognized_cycles_jun23)\n",
    "        create_reference_label_file(folder_path+\"/aug23_cycle.txt\",df_recognized_cycles_aug23)\n",
    "        create_reference_label_file(folder_path+\"/okt23_cycle.txt\",df_recognized_cycles_okt23)\n",
    "        create_reference_label_file(folder_path+\"/dec23_cycle.txt\",df_recognized_cycles_dec23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_exp2 = pd.DataFrame(columns=['state_classifier', \"detection mean\",'detection std',\n",
    "                                        'Abnormal mean F1-score','Abnormal std F1-score',\n",
    "                                        'Normal mean F1-score','Normal std F1-score',\n",
    "                                        'Overall mean F1-score','Overall std F1-score'])\n",
    "\n",
    "seeds=list(range(0,10))\n",
    "classifiers=[\"rf\",\"dt\",\"xtree\",\"mlp\"]\n",
    "\n",
    "dir = './results/reference_cycle_labels/'\n",
    "reference_path = dir\n",
    "collar = 202.75\n",
    "\n",
    "detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "detection_files_overall, f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[],[]\n",
    "\n",
    "for classifier_state in classifiers:\n",
    "    detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "    detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "    detection_files_overall, f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[],[]\n",
    "\n",
    "    for seed_cycle in seeds:\n",
    "        result_path = dir_exp2+classifier_state+\"/\"+str(seed_cycle)+\"/\"\n",
    "        f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal , f1score_normal, precision_normal, recall_normal= compute_classification_sedeval(reference_path,result_path,collar)\n",
    "        detection_file_overall = compute_detection_sedeval(reference_path,result_path,collar)\n",
    "        detection_files_overall.append(detection_file_overall)\n",
    "        f1score_files_overall.append(f1score_file)\n",
    "        precision_files_overall.append(precision_file)\n",
    "        recall_files_overall.append(recall_file)\n",
    "        f1score_files_abnormal.append(f1score_abnormal)\n",
    "        precision_files_abnormal.append(precision_abnormal)\n",
    "        recall_files_abnormal.append(recall_abnormal)\n",
    "        f1score_files_normal.append(f1score_normal)\n",
    "        precision_files_normal.append(precision_normal)\n",
    "        recall_files_normal.append(recall_normal)\n",
    "\n",
    "    print(\"---------- \"+classifier_state+\" ----------\")\n",
    "    print(\"DETECTION: \"+ str(np.mean(detection_files_overall)*100) +\" - \"+ str(np.std(detection_files_overall)*100) )\n",
    "    print(\"ABNORMAL:\")\n",
    "    print(\"F1-score: \"+ str(np.mean(f1score_files_abnormal)*100) +\" - \"+ str(np.std(f1score_files_abnormal)*100) )\n",
    "    print(\"Precision: \"+ str(np.mean(precision_files_abnormal)*100) +\" - \"+ str(np.std(precision_files_abnormal)*100) )\n",
    "    print(\"Recall: \"+ str(np.mean(recall_files_abnormal)*100) +\" - \"+ str(np.std(recall_files_abnormal)*100) )\n",
    "    print(\"NORMAL:\")\n",
    "    print(\"F1-score: \"+ str(np.mean(f1score_files_normal)*100) +\" - \"+ str(np.std(f1score_files_normal)*100) )\n",
    "    print(\"Precision: \"+ str(np.mean(precision_files_normal)*100) +\" - \"+ str(np.std(precision_files_normal)*100) )\n",
    "    print(\"Recall: \"+ str(np.mean(recall_files_normal)*100) +\" - \"+ str(np.std(recall_files_normal)*100) )\n",
    "    print(\"OVERALL:\")\n",
    "    print(\"F1-score: \"+ str(np.mean(f1score_files_overall)*100) +\" - \"+ str(np.std(f1score_files_overall)*100) )\n",
    "    print(\"Precision: \"+ str(np.mean(precision_files_overall)*100) +\" - \"+ str(np.std(precision_files_overall)*100) )\n",
    "    print(\"Recall: \"+ str(np.mean(recall_files_overall)*100) +\" - \"+ str(np.std(recall_files_overall)*100) )\n",
    "\n",
    "    dflocal = pd.DataFrame({'state_classifier':classifier_state, \n",
    "                            \"detection mean\": np.mean(detection_files_overall)*100,\n",
    "                            'detection std': np.std(detection_files_overall)*100,\n",
    "                            'Abnormal mean F1-score':np.mean(f1score_files_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_files_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_files_overall)*100,\n",
    "                            'Abnormal std F1-score':np.std(f1score_files_abnormal)*100,\n",
    "                            'Normal std F1-score':np.std(f1score_files_normal)*100,\n",
    "                            'Overall std F1-score':np.std(f1score_files_overall)*100},index=[0])\n",
    "        \n",
    "    df_results_exp2 = pd.concat([df_results_exp2, dflocal], ignore_index=True)\n",
    "\n",
    "\n",
    "classifiers=[\"xgboost\",\"nb\"]\n",
    "\n",
    "detection_files_abnormal, f1score_files_abnormal, precision_files_abnormal, recall_files_abnormal = [],[],[],[]\n",
    "detection_files_normal, f1score_files_normal, precision_files_normal, recall_files_normal = [],[],[],[]\n",
    "detection_files_overall, f1score_files_overall, precision_files_overall, recall_files_overall = [],[],[],[]\n",
    "\n",
    "for classifier_state in classifiers:\n",
    "    result_path = dir_exp2+classifier_state+\"/\"\n",
    "    f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal , f1score_normal, precision_normal, recall_normal= compute_classification_sedeval(reference_path,result_path,collar)\n",
    "    detection_file_overall = compute_detection_sedeval(reference_path,result_path,collar)\n",
    "\n",
    "    print(\"---------- \"+classifier_state+\" ----------\")\n",
    "    print(\"DETECTION: \"+ str(detection_file_overall*100) )\n",
    "    print(\"ABNORMAL:\")\n",
    "    print(\"F1-score: \"+ str(f1score_abnormal*100) )\n",
    "    print(\"Precision: \"+ str(precision_abnormal*100) )\n",
    "    print(\"Recall: \"+ str(recall_abnormal*100) )\n",
    "    print(\"NORMAL:\")\n",
    "    print(\"F1-score: \"+ str(f1score_normal*100) )\n",
    "    print(\"Precision: \"+ str(precision_normal*100) )\n",
    "    print(\"Recall: \"+ str(recall_normal*100) )\n",
    "    print(\"OVERALL:\")\n",
    "    print(\"F1-score: \"+ str(f1score_file*100)  )\n",
    "    print(\"Precision: \"+ str(precision_file*100) )\n",
    "    print(\"Recall: \"+ str(recall_file*100) )\n",
    "\n",
    "    dflocal = pd.DataFrame({'state_classifier':classifier_state,\n",
    "                            \"detection mean\": np.mean(detection_file_overall)*100, \n",
    "                            'Abnormal mean F1-score':np.mean(f1score_abnormal)*100,\n",
    "                            'Normal mean F1-score':np.mean(f1score_normal)*100,\n",
    "                            'Overall mean F1-score':np.mean(f1score_file)*100},index=[0])\n",
    "        \n",
    "    df_results_exp2 = pd.concat([df_results_exp2, dflocal], ignore_index=True)\n",
    "\n",
    "df_results_exp2.to_csv(dir_exp2 + 'experiment2_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment on MCU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17573/3634012107.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_state [y_state=='E']='B'\n",
      "/tmp/ipykernel_17573/3634012107.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_state [y_state=='E']='B'\n"
     ]
    }
   ],
   "source": [
    "df_dataset= pd.concat([data_csv_jun21, data_csv_okt21,data_csv_jan22,data_csv_april22])\n",
    "\n",
    "df_dataset[\"ref_label\"]= np.concatenate((true_label_jun21, true_label_okt21,true_label_jan22,true_label_april22), axis=0)    \n",
    "df_dataset[\"ref_label_cycle\"]= np.concatenate((true_label_cycle_jun21, true_label_cycle_okt21,true_label_cycle_jan22,true_label_cycle_april22), axis=0)    \n",
    "\n",
    "removed_indices = df_dataset[df_dataset['ref_label'].isnull()].index.tolist()\n",
    "df_dataset = df_dataset[df_dataset['ref_label'].notnull()]\n",
    "df_dataset=df_dataset.reset_index()\n",
    "\n",
    "# remove the recognized_label column added in the experiment1 or experiment2\n",
    "if 'recognized_label' in df_dataset.columns:\n",
    "    df_dataset = df_dataset.drop('recognized_label', axis=1)\n",
    "\n",
    "x = df_dataset[df_dataset.columns[1:-2]]\n",
    "y_cycle = df_dataset[df_dataset.columns[-1]]\n",
    "y_state = df_dataset[df_dataset.columns[-2]]\n",
    "\n",
    "y_state [y_state=='E']='B'\n",
    "\n",
    "# normalize feature to range [0;1]\n",
    "scaler = MinMaxScaler(clip=True)\n",
    "scaler.fit(x)\n",
    "x_train = pd.DataFrame(scaler.transform(x), columns=x.columns)\n",
    "\n",
    "y_state [y_state=='E']='B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance ds1 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value class-counts in Unbalanced dataset: {'A': 10566, 'B': 123236, 'C': 11444, 'D': 24298}\n",
      "Value class-porcentage in Unbalanced dataset: {'A': 6.232010569527674, 'B': 72.68673618647667, 'C': 6.7498702401736415, 'D': 14.331383003822019}\n",
      "Value class-counts in Balanced dataset: {0: 21132, 1: 30809, 2: 22888, 3: 24298}\n",
      "Value class-porcentage in Balanced dataset: {0: 21.31810707476268, 1: 31.080331292180734, 2: 23.089571963239077, 3: 24.511989669817506}\n"
     ]
    }
   ],
   "source": [
    "x_train_balanced, y_train_balanced,le = balance_dataset(x_train,y_state)\n",
    "\n",
    "# Print balanced dataset\n",
    "unique_values, counts = np.unique(y_state, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "print(\"Value class-counts in Unbalanced dataset:\",value_counts)\n",
    "print(\"Value class-porcentage in Unbalanced dataset:\",value_porcentages)\n",
    "\n",
    "unique_values, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "value_porcentages = dict(zip(unique_values, counts/sum(counts)*100))\n",
    "print(\"Value class-counts in Balanced dataset:\",value_counts)\n",
    "print(\"Value class-porcentage in Balanced dataset:\",value_porcentages)\n",
    "\n",
    "del unique_values,counts,value_counts,value_porcentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALED VALUES TO BE ADDED IN THE C-CODE\n",
      "Minimun:  [ 0.          0.          0.          0.          0.          0.\n",
      " -1.34333333  0.          0.          0.         -1.194      -1.37      ]\n",
      "Maximun:  [155.93        41.73        45.55        45.39333333 155.35333333\n",
      "  41.31333333 120.82333333  45.36       154.672       40.976\n",
      " 119.466      123.51      ]\n"
     ]
    }
   ],
   "source": [
    "print(\"SCALED VALUES TO BE ADDED IN THE C-CODE\")\n",
    "print(f\"Minimun: \",scaler.data_min_)\n",
    "print(f\"Maximun: \",scaler.data_max_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High-pressure</th>\n",
       "      <th>Low-pressure</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Speed_order3</th>\n",
       "      <th>High-pressure_order3</th>\n",
       "      <th>Low-pressure_order3</th>\n",
       "      <th>Diff-pressure_order3</th>\n",
       "      <th>Speed_order5</th>\n",
       "      <th>High-pressure_order5</th>\n",
       "      <th>Low-pressure_order5</th>\n",
       "      <th>Diff-pressure_order5</th>\n",
       "      <th>Diff-pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.332104</td>\n",
       "      <td>0.568258</td>\n",
       "      <td>0.436464</td>\n",
       "      <td>0.437485</td>\n",
       "      <td>0.333488</td>\n",
       "      <td>0.574119</td>\n",
       "      <td>0.240925</td>\n",
       "      <td>0.435835</td>\n",
       "      <td>0.335079</td>\n",
       "      <td>0.578888</td>\n",
       "      <td>0.242838</td>\n",
       "      <td>0.235758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.284168</td>\n",
       "      <td>0.319134</td>\n",
       "      <td>0.482399</td>\n",
       "      <td>0.478376</td>\n",
       "      <td>0.284036</td>\n",
       "      <td>0.321885</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>0.473490</td>\n",
       "      <td>0.283725</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.301045</td>\n",
       "      <td>0.294641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.196146</td>\n",
       "      <td>0.484783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201605</td>\n",
       "      <td>0.490399</td>\n",
       "      <td>0.058445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202642</td>\n",
       "      <td>0.494607</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>0.057335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.247804</td>\n",
       "      <td>0.675533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249710</td>\n",
       "      <td>0.682266</td>\n",
       "      <td>0.084011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252483</td>\n",
       "      <td>0.687964</td>\n",
       "      <td>0.090356</td>\n",
       "      <td>0.075673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.410463</td>\n",
       "      <td>0.834891</td>\n",
       "      <td>0.987706</td>\n",
       "      <td>0.990454</td>\n",
       "      <td>0.413747</td>\n",
       "      <td>0.842908</td>\n",
       "      <td>0.243192</td>\n",
       "      <td>0.990520</td>\n",
       "      <td>0.418893</td>\n",
       "      <td>0.849619</td>\n",
       "      <td>0.254031</td>\n",
       "      <td>0.234625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       High-pressure  Low-pressure         Speed  Speed_order3   \n",
       "count   99127.000000  99127.000000  99127.000000  99127.000000  \\\n",
       "mean        0.332104      0.568258      0.436464      0.437485   \n",
       "std         0.284168      0.319134      0.482399      0.478376   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.196146      0.484783      0.000000      0.000000   \n",
       "50%         0.247804      0.675533      0.000000      0.000000   \n",
       "75%         0.410463      0.834891      0.987706      0.990454   \n",
       "max         1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       High-pressure_order3  Low-pressure_order3  Diff-pressure_order3   \n",
       "count          99127.000000         99127.000000          99127.000000  \\\n",
       "mean               0.333488             0.574119              0.240925   \n",
       "std                0.284036             0.321885              0.299536   \n",
       "min                0.000000             0.000000              0.000000   \n",
       "25%                0.201605             0.490399              0.058445   \n",
       "50%                0.249710             0.682266              0.084011   \n",
       "75%                0.413747             0.842908              0.243192   \n",
       "max                1.000000             1.000000              1.000000   \n",
       "\n",
       "       Speed_order5  High-pressure_order5  Low-pressure_order5   \n",
       "count  99127.000000          99127.000000         99127.000000  \\\n",
       "mean       0.435835              0.335079             0.578888   \n",
       "std        0.473490              0.283725             0.323944   \n",
       "min        0.000000              0.000000             0.000000   \n",
       "25%        0.000000              0.202642             0.494607   \n",
       "50%        0.000000              0.252483             0.687964   \n",
       "75%        0.990520              0.418893             0.849619   \n",
       "max        1.000000              1.000000             1.000000   \n",
       "\n",
       "       Diff-pressure_order5  Diff-pressure  \n",
       "count          99127.000000   99127.000000  \n",
       "mean               0.242838       0.235758  \n",
       "std                0.301045       0.294641  \n",
       "min                0.000000       0.000000  \n",
       "25%                0.057998       0.057335  \n",
       "50%                0.090356       0.075673  \n",
       "75%                0.254031       0.234625  \n",
       "max                1.000000       1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data (DS2)\n",
    "df_testset= pd.concat([data_csv_jun23, data_csv_aug23,data_csv_okt23,data_csv_dec23])\n",
    "x_test_float = pd.DataFrame(scaler.transform(df_testset), columns=df_testset.columns,index=df_testset.index)\n",
    "\n",
    "x_train_float= x_train_balanced.copy()\n",
    "x_train_float.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xtree internal-state classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlearn import convert\n",
    "\n",
    "classifier=\"xtree\"\n",
    "seed=0\n",
    "dir_model_pred = \"./results/approach2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a state classifier using all data from DS1 and test it on DS2\n",
    "clf = train_state_supervised_classifier(classifier,x_train_float, y_train_balanced,seed)\n",
    "\n",
    "#test state-cycles\n",
    "y_predict = clf.predict(x_test_float)\n",
    "y_pred_smoothed = smooth_labels(y_predict,3)     #apply 3rd median filter\n",
    "y_recognized=le.inverse_transform(y_pred_smoothed.astype(int))\n",
    "df_temp=df_testset.copy()\n",
    "df_temp[\"recognized_label\"]=y_recognized\n",
    "df_temp=df_temp.reset_index()\n",
    "\n",
    "#classify duty-cycle\n",
    "df_recognized_states_jun23 = create_segments_state(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_temp)\n",
    "df_recognized_states_aug23 = create_segments_state(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_temp)\n",
    "df_recognized_states_okt23 = create_segments_state(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_temp)\n",
    "df_recognized_states_dec23 = create_segments_state(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_temp)\n",
    "\n",
    "folder_path = dir_model_pred+\"MCU/\"+classifier+\"/float\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "if flag_save_results:\n",
    "    create_reference_label_file(folder_path+\"/jun23_state.txt\",df_recognized_states_jun23)\n",
    "    create_reference_label_file(folder_path+\"/aug23_state.txt\",df_recognized_states_aug23)\n",
    "    create_reference_label_file(folder_path+\"/okt23_state.txt\",df_recognized_states_okt23)\n",
    "    create_reference_label_file(folder_path+\"/dec23_state.txt\",df_recognized_states_dec23)        \n",
    "\n",
    "    cmodel = convert(clf, method='inline',dtype='float')\n",
    "    cmodel.save(file = \"./results/c_code/approach2/xtree_float.h\", name='clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove in the header file the array \"static const EmlTreesNode clf_nodes[]\", which is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply threshold and classify duty-cycles\n",
    "\n",
    "#import classified state_labels\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "#form testset\n",
    "df_tmp=x_test_float.copy()\n",
    "df_tmp[\"recognized_label\"] = pd.concat([data_csv_jun23[\"recognized_label\"], data_csv_aug23[\"recognized_label\"],\n",
    "                                        data_csv_okt23[\"recognized_label\"],data_csv_dec23[\"recognized_label\"]])\n",
    "\n",
    "#apply detection threshold\n",
    "df_tmp.reset_index(inplace=True)\n",
    "df_tmp[\"detected_cycles\"]='No_cycle'\n",
    "threshold_speed = 2.5\n",
    "threshold_speed_normalized = (threshold_speed-scaler.data_min_[2])/scaler.data_range_[2]\n",
    "# threshold_speed_quant = (threshold_speed_normalized*(2**32 - 1)).astype('float')\n",
    "df_tmp.loc[df_tmp['Speed_order3'] > threshold_speed_normalized ,'detected_cycles'] = 'Cycle'\n",
    "\n",
    "#classify duty_cycle\n",
    "apply_heuristic_rules(df_tmp)\n",
    "\n",
    "#save results\n",
    "df_recognized_cycles_jun23 = create_segments_cycle_classified(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_testset=df_tmp)\n",
    "df_recognized_cycles_aug23 = create_segments_cycle_classified(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_testset=df_tmp)\n",
    "df_recognized_cycles_okt23 = create_segments_cycle_classified(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_testset=df_tmp)\n",
    "df_recognized_cycles_dec23 = create_segments_cycle_classified(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_testset=df_tmp)\n",
    "\n",
    "\n",
    "if flag_save_results:\n",
    "    create_reference_label_file(folder_path+\"/jun23_cycle.txt\",df_recognized_cycles_jun23)\n",
    "    create_reference_label_file(folder_path+\"/aug23_cycle.txt\",df_recognized_cycles_aug23)\n",
    "    create_reference_label_file(folder_path+\"/okt23_cycle.txt\",df_recognized_cycles_okt23)\n",
    "    create_reference_label_file(folder_path+\"/dec23_cycle.txt\",df_recognized_cycles_dec23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High-pressure</th>\n",
       "      <th>Low-pressure</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Speed_order3</th>\n",
       "      <th>High-pressure_order3</th>\n",
       "      <th>Low-pressure_order3</th>\n",
       "      <th>Diff-pressure_order3</th>\n",
       "      <th>Speed_order5</th>\n",
       "      <th>High-pressure_order5</th>\n",
       "      <th>Low-pressure_order5</th>\n",
       "      <th>Diff-pressure_order5</th>\n",
       "      <th>Diff-pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "      <td>99127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84.297366</td>\n",
       "      <td>144.528948</td>\n",
       "      <td>111.096977</td>\n",
       "      <td>111.352931</td>\n",
       "      <td>84.636567</td>\n",
       "      <td>145.991808</td>\n",
       "      <td>60.878641</td>\n",
       "      <td>110.926468</td>\n",
       "      <td>85.040251</td>\n",
       "      <td>147.222704</td>\n",
       "      <td>61.413308</td>\n",
       "      <td>59.544251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.338710</td>\n",
       "      <td>81.205404</td>\n",
       "      <td>122.810907</td>\n",
       "      <td>121.785839</td>\n",
       "      <td>72.311531</td>\n",
       "      <td>81.895703</td>\n",
       "      <td>76.427025</td>\n",
       "      <td>120.535658</td>\n",
       "      <td>72.231558</td>\n",
       "      <td>82.425757</td>\n",
       "      <td>76.771326</td>\n",
       "      <td>75.186669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>254.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       High-pressure  Low-pressure         Speed  Speed_order3   \n",
       "count   99127.000000  99127.000000  99127.000000  99127.000000  \\\n",
       "mean       84.297366    144.528948    111.096977    111.352931   \n",
       "std        72.338710     81.205404    122.810907    121.785839   \n",
       "min         0.000000      0.000000      0.000000      0.000000   \n",
       "25%        50.000000    123.000000      0.000000      0.000000   \n",
       "50%        63.000000    172.000000      0.000000      0.000000   \n",
       "75%       104.000000    212.000000    251.000000    252.000000   \n",
       "max       255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "       High-pressure_order3  Low-pressure_order3  Diff-pressure_order3   \n",
       "count          99127.000000         99127.000000          99127.000000  \\\n",
       "mean              84.636567           145.991808             60.878641   \n",
       "std               72.311531            81.895703             76.427025   \n",
       "min                0.000000             0.000000              0.000000   \n",
       "25%               51.000000           125.000000             14.000000   \n",
       "50%               63.000000           173.000000             21.000000   \n",
       "75%              105.000000           214.000000             62.000000   \n",
       "max              255.000000           255.000000            254.000000   \n",
       "\n",
       "       Speed_order5  High-pressure_order5  Low-pressure_order5   \n",
       "count  99127.000000          99127.000000         99127.000000  \\\n",
       "mean     110.926468             85.040251           147.222704   \n",
       "std      120.535658             72.231558            82.425757   \n",
       "min        0.000000              0.000000             0.000000   \n",
       "25%        0.000000             51.000000           126.000000   \n",
       "50%        0.000000             64.000000           175.000000   \n",
       "75%      252.000000            106.000000           216.000000   \n",
       "max      254.000000            255.000000           255.000000   \n",
       "\n",
       "       Diff-pressure_order5  Diff-pressure  \n",
       "count          99127.000000   99127.000000  \n",
       "mean              61.413308      59.544251  \n",
       "std               76.771326      75.186669  \n",
       "min                0.000000       0.000000  \n",
       "25%               14.000000      14.000000  \n",
       "50%               23.000000      19.000000  \n",
       "75%               64.000000      59.000000  \n",
       "max              255.000000     254.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data quantization\n",
    "x_train_uint8 = pd.DataFrame((x_train_balanced*(2**8 - 1)).astype('uint8'), columns=x.columns)\n",
    "x_test_uint8 = pd.DataFrame((x_test_float*(2**8 - 1)).astype('uint8'), columns=x.columns)\n",
    "\n",
    "x_train_uint8.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = train_state_supervised_classifier(classifier,x_train_uint8, y_train_balanced,seed)\n",
    "\n",
    "#test state-cycles\n",
    "y_predict = clf.predict(x_test_uint8)\n",
    "y_pred_smoothed = smooth_labels(y_predict,3)     #apply 3rd median filter\n",
    "y_recognized=le.inverse_transform(y_pred_smoothed.astype(int))\n",
    "df_temp=df_testset.copy()\n",
    "df_temp[\"recognized_label\"]=y_recognized\n",
    "df_temp=df_temp.reset_index()\n",
    "\n",
    "#classify duty-cycle\n",
    "df_recognized_states_jun23 = create_segments_state(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_temp)\n",
    "df_recognized_states_aug23 = create_segments_state(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_temp)\n",
    "df_recognized_states_okt23 = create_segments_state(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_temp)\n",
    "df_recognized_states_dec23 = create_segments_state(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_temp)\n",
    "\n",
    "folder_path = dir_model_pred+ \"/MCU/\"+classifier+\"/uint8\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "if flag_save_results:\n",
    "    create_reference_label_file(folder_path+\"/jun23_state.txt\",df_recognized_states_jun23)\n",
    "    create_reference_label_file(folder_path+\"/aug23_state.txt\",df_recognized_states_aug23)\n",
    "    create_reference_label_file(folder_path+\"/okt23_state.txt\",df_recognized_states_okt23)\n",
    "    create_reference_label_file(folder_path+\"/dec23_state.txt\",df_recognized_states_dec23)        \n",
    "\n",
    "    cmodel = convert(clf, method='inline',dtype='uint8_t')\n",
    "    cmodel.save(file= \"./results/c_code/approach2/xtree_uint8.h\", name='clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply threshold and classify duty-cycles\n",
    "\n",
    "#import classified state_labels\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/jun23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_jun23[\"recognized_label\"] = ndarray_labels(data_csv_jun23.index[0],data_csv_jun23.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/aug23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_aug23[\"recognized_label\"] = ndarray_labels(data_csv_aug23.index[0],data_csv_aug23.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/okt23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_okt23[\"recognized_label\"] = ndarray_labels(data_csv_okt23.index[0],data_csv_okt23.index[-1],aux,downsampled_freq)\n",
    "aux = df_timestamps(pd.read_csv(folder_path+\"/dec23_state.txt\",sep='\\t',names=[\"Time(Seconds)\",\"Length(Seconds)\",\"Label(string)\"]))\n",
    "data_csv_dec23[\"recognized_label\"] = ndarray_labels(data_csv_dec23.index[0],data_csv_dec23.index[-1],aux,downsampled_freq)\n",
    "\n",
    "#form testset\n",
    "df_tmp=x_test_uint8.copy()\n",
    "df_tmp[\"recognized_label\"] = pd.concat([data_csv_jun23[\"recognized_label\"], data_csv_aug23[\"recognized_label\"],\n",
    "                                        data_csv_okt23[\"recognized_label\"],data_csv_dec23[\"recognized_label\"]])\n",
    "\n",
    "#apply detection threshold\n",
    "df_tmp.reset_index(inplace=True)\n",
    "df_tmp[\"detected_cycles\"]='No_cycle'\n",
    "threshold_speed = 2.5\n",
    "threshold_speed_normalized = (threshold_speed-scaler.data_min_[2])/scaler.data_range_[2]\n",
    "threshold_speed_quant = (threshold_speed_normalized*(2**8 - 1)).astype('float')\n",
    "df_tmp.loc[df_tmp['Speed_order3'] > threshold_speed_quant ,'detected_cycles'] = 'Cycle'\n",
    "\n",
    "#classify duty_cycle\n",
    "apply_heuristic_rules(df_tmp)\n",
    "\n",
    "#save results\n",
    "df_recognized_cycles_jun23 = create_segments_cycle_classified(datetime.datetime(2023, 6, 1),datetime.datetime(2023, 7, 1),df_testset=df_tmp)\n",
    "df_recognized_cycles_aug23 = create_segments_cycle_classified(datetime.datetime(2023, 8, 1),datetime.datetime(2023, 9, 1),df_testset=df_tmp)\n",
    "df_recognized_cycles_okt23 = create_segments_cycle_classified(datetime.datetime(2023, 10, 1),datetime.datetime(2023, 11, 1),df_testset=df_tmp)\n",
    "df_recognized_cycles_dec23 = create_segments_cycle_classified(datetime.datetime(2023, 12, 1),datetime.datetime(2024, 1, 1),df_testset=df_tmp)\n",
    "\n",
    "if flag_save_results:\n",
    "    create_reference_label_file(folder_path+\"/jun23_cycle.txt\",df_recognized_cycles_jun23)\n",
    "    create_reference_label_file(folder_path+\"/aug23_cycle.txt\",df_recognized_cycles_aug23)\n",
    "    create_reference_label_file(folder_path+\"/okt23_cycle.txt\",df_recognized_cycles_okt23)\n",
    "    create_reference_label_file(folder_path+\"/dec23_cycle.txt\",df_recognized_cycles_dec23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- xtree - float ----------\n",
      "DETECTION: 99.16317991631799\n",
      "ABNORMAL:\n",
      "F1-score: 64.36781609195403\n",
      "Precision: 53.84615384615385\n",
      "Recall: 80.0\n",
      "NORMAL:\n",
      "F1-score: 87.76978417266189\n",
      "Precision: 94.72049689440993\n",
      "Recall: 81.76943699731903\n",
      "OVERALL:\n",
      "F1-score: 81.3807531380753\n",
      "Precision: 81.3807531380753\n",
      "Recall: 81.3807531380753\n",
      "---------- xtree - uint8 ----------\n",
      "DETECTION: 99.16317991631799\n",
      "ABNORMAL:\n",
      "F1-score: 64.88549618320612\n",
      "Precision: 54.14012738853503\n",
      "Recall: 80.95238095238095\n",
      "NORMAL:\n",
      "F1-score: 87.89625360230548\n",
      "Precision: 95.01557632398755\n",
      "Recall: 81.76943699731903\n",
      "OVERALL:\n",
      "F1-score: 81.58995815899581\n",
      "Precision: 81.58995815899581\n",
      "Recall: 81.58995815899581\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['quantization','state_classifier', \"detection mean\",\n",
    "                                    'Abnormal mean F1-score','Normal mean F1-score','Overall mean F1-score'])\n",
    "\n",
    "dir = './results/reference_cycle_labels/'\n",
    "reference_path = dir\n",
    "collar = 202.75\n",
    "\n",
    "classifiers=[\"xtree\"]\n",
    "quantizations=[\"float\",\"uint8\"]\n",
    "for classifier_state in classifiers:\n",
    "    for q in quantizations:\n",
    "        result_path = dir_model_pred + 'MCU/'+classifier_state+\"/\"+q+\"/\"\n",
    "        f1score_file, precision_file, recall_file, f1score_abnormal, precision_abnormal, recall_abnormal , f1score_normal, precision_normal, recall_normal= compute_classification_sedeval(reference_path,result_path,collar)\n",
    "        detection_file_overall = compute_detection_sedeval(reference_path,result_path,collar)\n",
    "\n",
    "        print(\"---------- \"+classifier_state+ \" - \"+q+\" ----------\")\n",
    "        print(\"DETECTION: \"+ str(detection_file_overall*100) )\n",
    "        print(\"ABNORMAL:\")\n",
    "        print(\"F1-score: \"+ str(f1score_abnormal*100) )\n",
    "        print(\"Precision: \"+ str(precision_abnormal*100) )\n",
    "        print(\"Recall: \"+ str(recall_abnormal*100) )\n",
    "        print(\"NORMAL:\")\n",
    "        print(\"F1-score: \"+ str(f1score_normal*100) )\n",
    "        print(\"Precision: \"+ str(precision_normal*100) )\n",
    "        print(\"Recall: \"+ str(recall_normal*100) )\n",
    "        print(\"OVERALL:\")\n",
    "        print(\"F1-score: \"+ str(f1score_file*100)  )\n",
    "        print(\"Precision: \"+ str(precision_file*100) )\n",
    "        print(\"Recall: \"+ str(recall_file*100) )\n",
    "\n",
    "        dflocal = pd.DataFrame({'quantization':q,\n",
    "                                'state_classifier':classifier_state, \n",
    "                                \"detection mean\": np.mean(detection_file_overall)*100,\n",
    "                                'Abnormal mean F1-score':np.mean(f1score_abnormal)*100,\n",
    "                                'Normal mean F1-score':np.mean(f1score_normal)*100,\n",
    "                                'Overall mean F1-score':np.mean(f1score_file)*100},index=[0])\n",
    "            \n",
    "        df_results = pd.concat([df_results, dflocal], ignore_index=True)\n",
    "\n",
    "df_results.to_csv(dir_model_pred + 'MCU/results_mcu.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
